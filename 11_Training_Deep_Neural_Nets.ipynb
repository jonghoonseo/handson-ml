{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11. Training Deep Neural Nets.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/jonghoonseo/handson-ml/blob/wip/11_Training_Deep_Neural_Nets.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "Kq9lUfpAAQOc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Chapter 11. 심층 신경망 훈련"
      ]
    },
    {
      "metadata": {
        "id": "sLxzwcMtAQOn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "복잡한 문제 해결을 위한 깊은 심층 신경망 훈련의 문제\n",
        "1. Gradients 문제 - Vanishing Gradients / Exploding Gradients <= 적절한 초기화와 활성화 함수, 배치 정규화\n",
        "2. 극단적으로 느린 학습 속도 <= 모델 재사용, 학습 동결, 캐싱\n",
        "3. Overfitting <= 규제"
      ]
    },
    {
      "metadata": {
        "id": "oN3pJH9KAQOs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### 재실행! ###\n",
        "\n",
        "# 파이썬 2와 파이썬 3 지원\n",
        "from __future__ import division, print_function, unicode_literals\n",
        "\n",
        "# 공통\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# 일관된 출력을 위해 유사난수 초기화\n",
        "def reset_graph(seed=42):\n",
        "    tf.reset_default_graph()\n",
        "    tf.set_random_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "# 맷플롯립 설정\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n",
        "\n",
        "# 한글출력 - Colab 에서 설정하는 방법 찾아야 함.\n",
        "def get_osx_hangul_font():\n",
        "    import matplotlib as mpl\n",
        "    import matplotlib.pyplot as plt\n",
        "    import matplotlib.font_manager as fm\n",
        "\n",
        "    print ('설정파일 위치: ', mpl.matplotlib_fname())\n",
        "    # OSX 의 설치 된 폰트를 가져오는 함수\n",
        "    font_list_mac = fm.OSXInstalledFonts()\n",
        "\n",
        "    nanum_fonts = [(f.name, f.fname) for f in fm.fontManager.ttflist if 'Myungjo' in f.name]\n",
        "    print(nanum_fonts)\n",
        "# plt.rcParams[\"font.family\"] = 'AppleMyungjo'\n",
        "# # plt.rcParams['font.family'] = 'NanumBarunGothic'\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# 그림을 저장할 폴더\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"deep\"\n",
        "\n",
        "! mkdir -p images/deep\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True):\n",
        "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format='png', dpi=300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hCSwVAybAQO1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 11.1 그래디언트 소실과 폭주 문제\n",
        "\n",
        "역전파 알고리즘(backpropagation algorithm)\n",
        "- 출력층에서 입력층으로 오차 그래디언트를 전파시키면서 진행\n",
        "- 알고리즘이 모든 파라미터에 대한 오차 함수의 그래디언트를 계산하면 Gradients Descent 단계에서 이 그래디언트를 이용하여 파라미터를 갱신함\n",
        "- 알고리즘이 하위층으로 진행됨에 따라 그래디언트는 점점 작아지는 경우가 많음: **Vanishing Gradients**\n",
        "![Vanishing Gradient](https://t1.daumcdn.net/cfile/tistory/2738F43B583AC4C933)\n",
        "- 반대로 그래디언트가 점점 커져 여러개의 층이 비정상적으로 큰 가중치로 갱신되어 발산: **Exploding Gradients** (RNN 에서 주로 나타남)\n",
        "\n",
        "##### Understanding the difficulty of training deep feedforward neural networks\n",
        "> Glorot, Xavier, and Yoshua Bengio. \"**Understanding the difficulty of training deep feedforward neural networks.**\" In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249-256. 2010.\n",
        "\n",
        "- 로지스틱 시그모이드 활성화 함수\n",
        "- 가중치 초기화 방법\n",
        "  - 평균이 0, 표준편차가 1인 정규분포를 사용한 무작위 초기화\n",
        "- 이 조합으로 학습했을 때, **출력**의 분산이 **입력**의 분산보다 더 큼\n",
        "- 신경망의 위쪽으로 갈수록 분산이 계속 커져 가장 높은층에서는 Activation Function 이 **0이나 1**로 수렴함\n",
        "![LogisticActivation](https://github.com/jonghoonseo/handson-ml/blob/wip/images/deep/sigmoid_saturation_plot.png?raw=1)\n",
        "    - 로지스틱 함수의 **평균**이 0이 아니라 **0.5**\n",
        "    - 하이퍼볼릭 탄젠트는 평균이 0이므로 조금 더 나음\n",
        "    - 입력의 절대값이 크면 0이나 1로 수렴 => 기울기가 0에 매우 가까움 => backpropagation 시 propagate 할 그래디언트가 거의 없고, 최상위층부터 backpropagate 되면서 점차 약해져 실제로 아래쪽 층에는 아무것도 도달하지 않음\n",
        "    - 로지스틱 함수의 도함수는 $\\sigma(1-\\sigma)$ 이므로, 함수의 값이 0이나 1에 가까우면 도함수의 결과가 매우 작아지고, 층이 거듭될수록 그 값이 더 작아짐\n",
        "    "
      ]
    },
    {
      "metadata": {
        "id": "LKkaWWsvAQO3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "outputId": "654e097d-eff9-4fbb-d751-b1f9c5c4a91a"
      },
      "cell_type": "code",
      "source": [
        "def logit(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "z = np.linspace(-5, 5, 200)\n",
        "logit_z = logit(z)\n",
        "\n",
        "plt.plot([-5, 5], [0, 0], 'k-')\n",
        "plt.plot([-5, 5], [1, 1], 'k--')\n",
        "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
        "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
        "plt.plot(z, logit(z), \"b-\", linewidth=2)\n",
        "\n",
        "props = dict(facecolor='black', shrink=0.1)\n",
        "plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n",
        "plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
        "plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\")\n",
        "\n",
        "# print mean\n",
        "mean = sum(logit_z) / len(logit_z)\n",
        "print('Mean: ', mean)\n",
        "plt.plot([mean, mean], [0, logit(mean)], 'r--')\n",
        "plt.annotate('Mean', xytext=(mean-1, logit(mean+1)), xy=(mean,logit(mean)), arrowprops=props, fontsize=14, ha=\"center\")\n",
        "\n",
        "plt.grid(True)\n",
        "plt.title(\"Sigmoid activation function\", fontsize=14)\n",
        "plt.axis([-5, 5, -0.2, 1.2])\n",
        "\n",
        "save_fig(\"sigmoid_saturation_plot\")\n",
        "plt.show()"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean:  0.5000000000000002\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSgghhF/6f6A2gaYsASbxAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fdfb0703860>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "MXm3okfsAQPC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 11.1.1 Xavier 초기화와 He 초기화\n",
        "\n",
        "- 예측을 할 때는 정방향으로,<br/>\n",
        "역전파할 때는 역방향으로 신호가 적절하게 흘러야 함\n",
        "- 신호가 죽거나 폭주/소멸하지 않아야 함\n",
        "- **각 층의 출력에 대한 분산이 입력에 대한 분산과 동일해야 함!**\n",
        "\n",
        "----\n",
        "**식 11-1: Xavier 초기화 (로지스틱 활성화 함수를 사용했을 때)**\n",
        "\n",
        "> $\n",
        "\\begin{split}\n",
        "& \\text{평균이 0이고 표준 편차 }\n",
        "\\sigma = \\sqrt{\\dfrac{2}{n_\\text{inputs} + n_\\text{outputs}}} \\text{ 인 정규분포}\\\\\n",
        "& \\text{또는 }\n",
        "r = \\sqrt{\\dfrac{6}{n_\\text{inputs} + n_\\text{outputs}}} \\text{ 일 때 } -r \\text{ 과 } +r \\text{ 사이의 균등분포}\n",
        "\\end{split}\n",
        "$\n",
        "\n",
        "단, $n_\\text{inputs}$와 $n_\\text{outputs}$는 가중치를 초기화하려는 층의 입력/출력 연결의 개수 (fan-in/out)\n",
        "\n",
        "----\n",
        "\n",
        "**입력의 개수와 출력의 개수가 비슷하면($n_\\text{inputs} \\approx n_\\text{outputs}$) 더 간단한 공식 사용 가능**\n",
        "\n",
        "> 예를 들면, $ \\sigma = \\dfrac{1}{\\sqrt{n_\\text{inputs}}} $ 또는 $ r = \\dfrac{\\sqrt{3}}{\\sqrt{n_\\text{inputs}}} $\n",
        "\n",
        "----\n",
        "\n",
        "**He initialization**: ReLU 활성화 함수 및 변종들을 위한 초기화 variance_scaling_initializer전략\n",
        "\n",
        "----\n",
        "\n",
        "**표 11-1: 활성화 함수 종류에 따른 초기화 매개변수**\n",
        "\n",
        "| Activation Func. | Uniform Distribution________ | Normal Distribution________ |\n",
        "|--------------|-------------------|-----------------------------------|\n",
        "| Logistic | $ r = \\sqrt{\\dfrac{6}{n_\\text{inputs} + n_\\text{outputs}}} $ | $ \\sigma = \\sqrt{\\dfrac{2}{n_\\text{inputs} + n_\\text{outputs}}} $ |\n",
        "| Hyperbolic tangent | $ r = 4 \\sqrt{\\dfrac{6}{n_\\text{inputs} + n_\\text{outputs}}} $ | $ \\sigma = 4 \\sqrt{\\dfrac{2}{n_\\text{inputs} + n_\\text{outputs}}} $ |\n",
        "| ReLU (and its variants) |$ r = \\sqrt{2} \\sqrt{\\dfrac{6}{n_\\text{inputs} + n_\\text{outputs}}} $ | $ \\sigma = \\sqrt{2} \\sqrt{\\dfrac{2}{n_\\text{inputs} + n_\\text{outputs}}} $|\n",
        "\n",
        "`tf.layer.dense()` 함수는 기본적으로 **Xavier 초기화(with uniform distribution)** 을 사용함 이를 `tf.variance_scaling_initializer()` 함수를 이용하여 아래와 같이 변경 가능\n",
        ">```\n",
        "he_init = tf.variance_scaling_initializer()\n",
        "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
        "                          kernel_initializer=he_init, name=\"hidden1\")\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "RSy22yjWAQPE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### 재실행! ###\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pC6PhcPlAQPJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### 재실행! ###\n",
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 * 28  # MNIST\n",
        "n_hidden1 = 300\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "\n",
        "he_init = tf.variance_scaling_initializer()\n",
        "# ReLU 활성화 함수, He 초기화\n",
        "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
        "                          kernel_initializer=he_init, name=\"hidden1\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S4Hp5UOeAQPb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 11.1.2 수렴하지 않는 활성화 함수 (Nonsaturating Activation Functions)\n",
        "\n",
        "- (Glorot & Bengio, 2010)의 insight 중 하나는, \"**활성화 함수를 잘못 선택하면 그래디언트의 소실이나 폭주로 이어질 수 있다**\"는 것\n",
        "  - 그 전에는 생물학적 뉴런과 유사한 Sigmoid 함수가 최선의 선택일 것이라고 추측\n",
        "- ReLU\n",
        "  - 특정 양숫값에 수렴하지 않을 뿐 아니라, 계산도 빠르다는 장점\n",
        "  - 하지만... *dying ReLUs problem*\n",
        "    - 훈련하는 동안 일부 뉴런이 0 이외의 값을 출력하지 않음\n",
        "    - 큰 학습률을 사용하면 뉴런 절반이 죽어있기도 함\n",
        "    - 학습 도중 가중치가 바뀌어 가중치 합이 음수가 되면, 그 다음부터는 0을 출력함 - 다시 살아나지 못함...\n",
        "- **LeakyReLU(새는 ReLU)**\n",
        "  - dying ReLUs 문제 해결\n",
        "  - $LeakyReLU_\\alpha(z) = max(\\alpha z,z)$\n",
        "  ![LeakyReLU](https://github.com/jonghoonseo/handson-ml/blob/wip/images/deep/leaky_relu_plot.png?raw=1)\n",
        "  - $\\alpha$\n",
        "    - 새는 정도(how much the function 'leaks')를 나타냄\n",
        "    - $z<0$일 때 이 함수의 기울기\n",
        "    - 일반적으로 $0.01$ <br/>\n",
        "      $0.2$ 가 더 나은 성능을 내는 것으로 보임\n",
        "    - ReLU가 절대 죽지 않고 다시 깨어날 가능성 부여\n",
        "  - (Xu, Wang, Chen, & Li, 2015)\n",
        "    - **LeakyReLU 가 항상 ReLU보다 성능이 높음**\n",
        "    - RReLU (Randomized leaky ReLU)\n",
        "      - 학습 시에는 범위 내에서 $\\alpha$를 무작위로 선택하고, 테스트 시에는 평균 사용\n",
        "      - 잘 동작했으며, overfitting 을 줄이는 Regulation 의 역할\n",
        "    - PReLU (Parametric leaky ReLU)\n",
        "      - 훈련 동안 $\\alpha$가 학습됨 (하이퍼파라미터가 아니라 역전파에 의해 변경됨)\n",
        "      - 대규모 이미지 데이터셋에서는 ReLU보다 성능이 앞서지만, 소규모 데이터셋에서는 overfitting 될 위험\n",
        "          "
      ]
    },
    {
      "metadata": {
        "id": "CjBjVfaKAQPc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "71cd09df-efd7-45cc-a914-a350b518cac6"
      },
      "cell_type": "code",
      "source": [
        "def leaky_relu(z, alpha=0.01):\n",
        "    return np.maximum(alpha*z, z)\n",
        "\n",
        "plt.plot([-5, 5], [0, 0], 'k-')\n",
        "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
        "plt.plot(z, leaky_relu(z, 0), \"g--\", linewidth=2)\n",
        "plt.plot(z, leaky_relu(z, 0.05), \"b:\", linewidth=2)\n",
        "plt.grid(True)\n",
        "props = dict(facecolor='black', shrink=0.1)\n",
        "plt.annotate('ReLU', xytext=(-1.5, 0.5), xy=(-3, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
        "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
        "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
        "plt.axis([-5, 5, -0.5, 4.2])\n",
        "\n",
        "save_fig(\"leaky_relu_plot\")\n",
        "plt.show()"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fdfafe616d8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "8GtKvWdpAQPg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Leaky ReLU를 이용한 tensorflow 학습"
      ]
    },
    {
      "metadata": {
        "id": "xf8FnHhsAQPh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### 재실행! ###\n",
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 * 28  # MNIST\n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 100\n",
        "n_outputs = 10\n",
        "\n",
        "# LeakyReLU 정의 - tf 1.4 에 추가됨 tf.nn.leaky_relu()\n",
        "def leaky_relu(z, name=None):\n",
        "    return tf.maximum(0.01 * z, z, name=name)\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
        "\n",
        "# define network\n",
        "with tf.name_scope(\"dnn\"):\n",
        "    # 28*28 -> 300 (leaky_relu)\n",
        "    hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")\n",
        "    # 300 -> 100 (leaky_relu)\n",
        "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=leaky_relu, name=\"hidden2\")\n",
        "    # 100 -> 10\n",
        "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
        "\n",
        "# loss\n",
        "# https://tensorflowkorea.gitbooks.io/tensorflow-kr/content/g3doc/tutorials/mnist/tf/\n",
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
        "    \n",
        "learning_rate = 0.01\n",
        "\n",
        "# train\n",
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    training_op = optimizer.minimize(loss)\n",
        "    \n",
        "# evaluation\n",
        "with tf.name_scope(\"eval\"):\n",
        "    # in_top_k(predictions, targets, k, name=None): Says whether the targets are in the top K predictions.\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)   # tf.argmax(logits)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "    \n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qOe3SDmjAQPj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "데이터 로드:\n",
        "\n",
        "주의: `tf.examples.tutorials.mnist`은 삭제될 예정이므로 대신 `tf.keras.datasets.mnist`를 사용하겠습니다."
      ]
    },
    {
      "metadata": {
        "id": "XuKQ8LvYAQPj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### 재실행! ###\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
        "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
        "y_train = y_train.astype(np.int32)\n",
        "y_test = y_test.astype(np.int32)\n",
        "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
        "y_valid, y_train = y_train[:5000], y_train[5000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kxy02Zv7AQPo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "1bb3ac48-3644-45bf-b903-bd625dbd12ce"
      },
      "cell_type": "code",
      "source": [
        "### 재실행! ###\n",
        "def shuffle_batch(X, y, batch_size):\n",
        "    rnd_idx = np.random.permutation(len(X))\n",
        "    n_batches = len(X) // batch_size\n",
        "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
        "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
        "        yield X_batch, y_batch\n",
        "        \n",
        "n_epochs = 40\n",
        "batch_size = 50\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        if epoch % 5 == 0:\n",
        "            acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "            acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "            print(epoch, \"배치 데이터 정확도:\", acc_batch, \"검증 세트 정확도:\", acc_valid)\n",
        "\n",
        "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 배치 데이터 정확도: 0.86 검증 세트 정확도: 0.9044\n",
            "5 배치 데이터 정확도: 0.94 검증 세트 정확도: 0.9494\n",
            "10 배치 데이터 정확도: 0.92 검증 세트 정확도: 0.9652\n",
            "15 배치 데이터 정확도: 0.94 검증 세트 정확도: 0.9712\n",
            "20 배치 데이터 정확도: 1.0 검증 세트 정확도: 0.9764\n",
            "25 배치 데이터 정확도: 1.0 검증 세트 정확도: 0.9776\n",
            "30 배치 데이터 정확도: 0.98 검증 세트 정확도: 0.9782\n",
            "35 배치 데이터 정확도: 1.0 검증 세트 정확도: 0.9786\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dL9s7aZeAQPs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**ELU (Exponential Linear Unit)**\n",
        "- (Clevert, Unterthiner, & Hochreiter, 2015)\n",
        "- 다른 모든 ReLU 변종의 성능을 앞지름\n",
        "- 학습 시간 감소\n",
        "- **식 11-2: ELU 활성화 함수**\n",
        ">$\n",
        "\\operatorname{ELU}_\\alpha(z) =\n",
        "\\begin{cases}\n",
        "\\alpha(\\exp(z) - 1) & z < 0 \\text{ 일 때}\\\\\n",
        "z & z \\ge 0 \\text{ 일 때}\n",
        "\\end{cases}\n",
        "$\n",
        "\n",
        "단, $\\alpha$ 는 $z$가 큰 음숫값일 때, ELU 가 수렴할 값. 보통 1로 설정\n",
        "\n",
        "<!-- ![ELU activation function](./images/deep/elu_plot.png) -->\n",
        "\n",
        "- ReLU 와의 차이점\n",
        "  1. $z < 0$ 일 때, 음수가 들어오므로, 평균 출력이 0에 더 가까움 -> Vanishing Gradients 문제를 완화해 줌. \n",
        "  2. $z < 0$ 이라도, 그래디언트가 0이 아니므로 죽은 뉴런을 만들지 않음\n",
        "  3. $\\alpha = 1$일 때, $z = 0$에서 급격히 변동하지 않고, $z = 0$을 포함해 모든 구간에서 매끄러우므로, 경사하강법의 속도를 높임 (ELU의 도함수는 $z<0$일 때, $\\alpha(z)$이고, $z \\geq 0$ 일 때, 1 이므로, $\\alpha \\neq 1$인 경우 $z = 0$에서 불연속)\n",
        "  \n",
        "- ELU의 단점\n",
        "  - 지수함수를 사용하기 때문에 계산이 느림\n",
        "  - 학습 동안에는 수렴 속도가 빠르므로 상쇄되나, <br/>\n",
        "    테스트 시에는 ReLU보다 느림"
      ]
    },
    {
      "metadata": {
        "id": "MA_KtBZsAQPt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "ac1b6f15-d66e-417a-fc1a-2fbd747b1a4d"
      },
      "cell_type": "code",
      "source": [
        "def elu(z, alpha=1):\n",
        "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)\n",
        "\n",
        "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
        "plt.plot([-5, 5], [0, 0], 'k-')\n",
        "plt.plot([-5, 5], [-1, -1], 'k--')\n",
        "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
        "plt.grid(True)\n",
        "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
        "plt.axis([-5, 5, -2.2, 3.2])\n",
        "\n",
        "save_fig(\"elu_plot\")\n",
        "plt.show()"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fdfafe91278>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "0USVGiw4Dm8g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "텐서플로에서 ELU를 구현하는 것은 간단합니다. 층을 구성할 때 활성화 함수에 지정하기만 하면 됩니다:"
      ]
    },
    {
      "metadata": {
        "id": "HrfuaYaJDruO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "\n",
        "# ELU 로 활성화: activation=tf.nn.elu\n",
        "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.elu, name=\"hidden1\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "laM_hfy-Dqdz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 정리 - 어떤 활성화 함수를 선택할 것인가?\n",
        "\n",
        "| Case | Activation Function |\n",
        "|-----|-------|\n",
        "| 일반적|  ELU > LeakyReLU(and variants) > ReLU > tanh > Logistic |\n",
        "| 실행 속도(Runtime Performance)가 중요 | LeakyReLU > ELU |\n",
        "| Overfitting 되었다면 | RReLU |\n",
        "| Training Set 이 아주 크다면 | PReLU |\n",
        "| 2017년 10월 구글 브레인팀 (Ramachandran, Zoph, & Le, 2018) | Swish $\\text{Swish}(z) = z \\text{sigmoid}(z)$ |\n",
        "\n",
        "이 경우, **He 초기화** 사용\n"
      ]
    },
    {
      "metadata": {
        "id": "lYUehedVAQPw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### References\n",
        "\n",
        "(Clevert, Unterthiner, & Hochreiter, 2015) Clevert, D.-A., Unterthiner, T., & Hochreiter, S. (2015). Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs). ArXiv:1511.07289 [Cs]. Retrieved from http://arxiv.org/abs/1511.07289\n",
        "\n",
        "(Glorot & Bengio, 2010) Glorot, X., & Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. In In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS’10). Society for Artificial Intelligence and Statistics.\n",
        "\n",
        "(Ramachandran, Zoph, & Le, 2018) Ramachandran, P., Zoph, B., & Le, Q. (2018). Searching for Activation Functions. Retrieved from https://arxiv.org/pdf/1710.05941.pdf\n",
        "\n",
        "(Xu, Wang, Chen, & Li, 2015) Xu, B., Wang, N., Chen, T., & Li, M. (2015). Empirical Evaluation of Rectified Activations in Convolutional Network. ArXiv:1505.00853 [Cs, Stat]. Retrieved from http://arxiv.org/abs/1505.00853\n",
        "\n",
        "\n",
        "https://pozalabs.github.io/Activation_Function/\n"
      ]
    },
    {
      "metadata": {
        "id": "df5BfjGULK4o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 11.1.3 배치 정규화 (Batch Normalization)\n",
        "\n",
        "ReLU variation + He Initialization => 학습 초기 단계에서 그래디언트 문제 해결. but, 훈련하는 동안 재발생 가능\n",
        "\n",
        "#### BN (Batch Normalization)\n",
        "- (Ioffe & Szegedy, 2015)\n",
        "- Goal\n",
        "  - 불안정한 그래디언트 문제 해결\n",
        "  - 더 일반적으로, **내부 공변량 변화(Internal Covariate Shift)** 문제 - 훈련하는 동안 이전 층의 파라미터가 변함에 따라 각 층에 들어오는 입력의 분포가 변화되는 문제 해결\n",
        "- 연산\n",
        "  - 각 층에서 활성화 함수를 통과하기 전에 모델에 연산을 하나 추가\n",
        "  - **입력 데이터의 평균을 0으로** 만들고 정규화한 다음, 각 층에서 두 개의 새로운 파라미터로 결괏값의 스케일을 조정하고 이동시킴.\n",
        "  - 파라미터 1: 스케일 조정 <br/>\n",
        "    파라미터 2: 이동\n",
        "  - 모델이 층마다 입력 데이터의 최적 스케일과 평균을 학습함.\n",
        "  - 현재 미니배치에서 입력의 평균과 표준편차를 평가함.\n",
        "  - **Equation 11-3: 배치 정규화 알고리즘**\n",
        "$$\n",
        "\\begin{split}\n",
        "1.\\quad & \\mathbf{\\mu}_B = \\dfrac{1}{m_B}\\sum\\limits_{i=1}^{m_B}{\\mathbf{x}^{(i)}}\\\\\n",
        "2.\\quad & {\\mathbf{\\sigma}_B}^2 = \\dfrac{1}{m_B}\\sum\\limits_{i=1}^{m_B}{(\\mathbf{x}^{(i)} - \\mathbf{\\mu}_B)^2}\\\\\n",
        "3.\\quad & \\hat{\\mathbf{x}}^{(i)} = \\dfrac{\\mathbf{x}^{(i)} - \\mathbf{\\mu}_B}{\\sqrt{{\\mathbf{\\sigma}_B}^2 + \\epsilon}}\\\\\n",
        "4.\\quad & \\mathbf{z}^{(i)} = \\gamma \\hat{\\mathbf{x}}^{(i)} + \\beta\n",
        "\\end{split}\n",
        "$$\n",
        "    - $\\mathbf{\\mu}_B$: 미니배치 $B$에 대해 평가하여 관측한 평균\n",
        "    - $\\mathbf{\\sigma}_B$: 미니배치 $B$에 대해 평가하여 관측한 표준편차\n",
        "    - $m_B$: 미니배치에 있는 샘플 수\n",
        "    - $\\hat{\\mathbf{x}}^{(i)}$: 평균이 0이고, 정규화된 입력\n",
        "    - $\\gamma$: 스케일 파라미터\n",
        "    - $\\beta$: 이동(편향) 파라미터\n",
        "    - $\\epsilon$: 분모가 0이 되는 것을 막는 작은 숫자 (안전을 위한 항 smoothing term)\n",
        "    - $\\mathbf{z}^{(i)}$: BN연산의 출력. 스케일 조정하고, 이동시킨 결과\n",
        "    \n",
        "  - **테스트 시** 에는 미니배치가 없으므로, 전체 훈련 세트의 평균과 표준편차를 대신 사용\n",
        "  - BN 층마다 $\\gamma$(스케일), $\\beta$(이동), $\\mu$(평균), $\\sigma$(표준편차) 네 개의 파라미터가 학습됨\n",
        "- 효과\n",
        "  - 성능 향상.\n",
        "  - tanh이나 로지스틱 활성화 함수 같이 수렴되는 활성화 함수를 사용해도 그래디언트 소실 문제가 크게 감소됨\n",
        "  - 가중치 초기화에 덜 민감해짐\n",
        "  - 큰 학습률을 사용할 수 있어, 학습 속도를 크게 개선\n",
        "  - overfitting 개선: 규제(Regularation)와 같은 역할을 하여, 다른 규제 기법의 필요성 줄여줌.(부수효과이므로, 드롭아웃을 함께 사용하는 것이 좋음)\n",
        "  \n",
        "- 한계\n",
        "  - 모델의 복잡도 증가\n",
        "  - 예측 속도 저하\n",
        "    - 예측이 매우 빨라야 한다면, ELU+He 초기화 만으로 얼마나 잘 수행되는지 먼저 확인\n",
        "  \n",
        "    "
      ]
    },
    {
      "metadata": {
        "id": "57fOS53jUHn5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 텐서플로로 배치 정규화 구현\n",
        "\n",
        "1. `tf.nn.batch_normalization()`\n",
        "  - 평균과 표준편차를 직접 계산해 매개변수로 전달해야 함.\n",
        "  - 스케일 조정과 이동을 위한 파라미터를 직접 생성하고 전달해야 함.\n",
        "2. `tf.layers.batch_normalization()`\n",
        "  - 사용이 편리"
      ]
    },
    {
      "metadata": {
        "id": "JwZeHOo2UphO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "n_inputs = 28 * 28\n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 100\n",
        "n_outputs = 10\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "\n",
        "# training: True, validation/testing: False\n",
        "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "\n",
        "# Fully Connected Layer 1: 28*28 -> 300 / 활성화 함수 없음!!!\n",
        "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
        "# Batch Normalization Layer 1: 300 -> 300\n",
        "bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
        "bn1_act = tf.nn.elu(bn1)\n",
        "\n",
        "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
        "bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
        "bn2_act = tf.nn.elu(bn2)\n",
        "\n",
        "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
        "logits = tf.layers.batch_normalization(logits_before_bn, training=training,\n",
        "                                       momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T5tcYWQoV0Wk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- `tf.placeholder_with_default`: training 여부에 따라 True / False 가짐. 13째 줄의 False 는 default value\n",
        "- fully connected layer 와 batch layer 가 번갈아 나옴\n",
        "- batch layer 이후에 activation function 을 사용하므로, fully connected layer 에서는 활성화 함수 없음.\n",
        "  - 하지만, 연구자에 따라 activation function 이후에 BN 수행하는 것이 좋다는 의견도 있음\n",
        "- BN알고리즘은 **지수 감수(exponential decay)** 를 사용해 이동 평균 계산\n",
        "  - 이동 평균 $\\hat{v}$은 아래와 같이 갱신됨 <br/>\n",
        "  $ \\hat{v} \\gets \\hat{v} \\times \\text{momentum} + v \\times (1 - \\text{momentum}) $\n",
        "  - 적절한 모멘텀 값은 일반적으로 1에 가까움\n",
        "  - 데이터셋이 크고 미니 배치가 작을 경우 9를 더 넣어 1에 더 가깝게 함\n",
        "  "
      ]
    },
    {
      "metadata": {
        "id": "pSib5w0lY6j3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "\n",
        "\n",
        "# functools.partial 이용하여 중복 매개변수 제거\n",
        "from functools import partial\n",
        "\n",
        "my_batch_norm_layer = partial(tf.layers.batch_normalization,\n",
        "                              training=training, momentum=0.9)\n",
        "\n",
        "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
        "bn1 = my_batch_norm_layer(hidden1)\n",
        "bn1_act = tf.nn.elu(bn1)\n",
        "\n",
        "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
        "bn2 = my_batch_norm_layer(hidden2)\n",
        "bn2_act = tf.nn.elu(bn2)\n",
        "\n",
        "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
        "logits = my_batch_norm_layer(logits_before_bn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "icaoygSxZvBl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "각 층에 ELU 활성화 함수와 배치 정규화를 사용하여 MNIST를 위한 신경망을 만듭니다:"
      ]
    },
    {
      "metadata": {
        "id": "sWSeHmeGZzMe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "batch_norm_momentum = 0.9\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
        "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "# training = tf.placeholder(tf.bool, shape=(), name='training')\n",
        "\n",
        "with tf.name_scope(\"dnn\"):\n",
        "    he_init = tf.variance_scaling_initializer()\n",
        "\n",
        "    my_batch_norm_layer = partial(\n",
        "            tf.layers.batch_normalization,\n",
        "            training=training,\n",
        "            momentum=batch_norm_momentum)\n",
        "\n",
        "    my_dense_layer = partial(\n",
        "            tf.layers.dense,\n",
        "            kernel_initializer=he_init)\n",
        "\n",
        "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
        "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
        "    \n",
        "    hidden2 = my_dense_layer(bn1, n_hidden2, name=\"hidden2\")\n",
        "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
        "    \n",
        "    logits_before_bn = my_dense_layer(bn2, n_outputs, name=\"outputs\")\n",
        "    logits = my_batch_norm_layer(logits_before_bn)\n",
        "\n",
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
        "\n",
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    training_op = optimizer.minimize(loss)\n",
        "\n",
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "    \n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MnJjAr9DZ-9b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "노트: 배치 정규화를 위해 별도의 업데이트 연산을 실행해 주어야 합니다(`sess.run([training_op, extra_update_ops],...`)."
      ]
    },
    {
      "metadata": {
        "id": "Z-KNukUtaDDM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "a9956ad1-09d7-4ebe-d2bd-efb19a656487"
      },
      "cell_type": "code",
      "source": [
        "n_epochs = 20\n",
        "batch_size = 200\n",
        "\n",
        "# moving_mean과 moving_variance를 업데이트\n",
        "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "            # Batch Normalization 을 위해 수정\n",
        "            # training_op 와 extra_update_ops 계산\n",
        "            # 학습 과정이므로, training은 True 로 설정\n",
        "            sess.run([training_op, extra_update_ops],\n",
        "                     feed_dict={training: True, X: X_batch, y: y_batch})\n",
        "        accuracy_val = accuracy.eval(feed_dict={training: False, X: X_valid, y: y_valid})\n",
        "        print(epoch, \"검증 세트 정확도:\", accuracy_val)\n",
        "\n",
        "    save_path = saver.save(sess, \"./my_BN_model_final.ckpt\")"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 검증 세트 정확도: 0.8952\n",
            "1 검증 세트 정확도: 0.9202\n",
            "2 검증 세트 정확도: 0.9318\n",
            "3 검증 세트 정확도: 0.9422\n",
            "4 검증 세트 정확도: 0.9468\n",
            "5 검증 세트 정확도: 0.954\n",
            "6 검증 세트 정확도: 0.9568\n",
            "7 검증 세트 정확도: 0.96\n",
            "8 검증 세트 정확도: 0.962\n",
            "9 검증 세트 정확도: 0.9638\n",
            "10 검증 세트 정확도: 0.9662\n",
            "11 검증 세트 정확도: 0.9682\n",
            "12 검증 세트 정확도: 0.9672\n",
            "13 검증 세트 정확도: 0.9696\n",
            "14 검증 세트 정확도: 0.9706\n",
            "15 검증 세트 정확도: 0.9704\n",
            "16 검증 세트 정확도: 0.9718\n",
            "17 검증 세트 정확도: 0.9726\n",
            "18 검증 세트 정확도: 0.9738\n",
            "19 검증 세트 정확도: 0.9742\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cGUyu72MhWLa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "훈련을 더 오래하면 정확도가 높아지겠지만 이런 얕은 신경망에서는 배치 정규화와 ELU가 큰 효과를 내지 못합니다. 대부분 심층 신경망에서 빛을 발합니다."
      ]
    },
    {
      "metadata": {
        "id": "oRCVyS0kLxYd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### References\n",
        "\n",
        "(Ioffe & Szegedy, 2015) Ioffe, S., & Szegedy, C. (2015). Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. Retrieved from https://arxiv.org/abs/1502.03167\n"
      ]
    },
    {
      "metadata": {
        "id": "SutNNnWYhk3k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 11.1.4 그래디언트 클리핑\n",
        "\n",
        "- Exploding Gradients 문제 해결\n",
        "- Backpropagation 시 일정 임계값을 넘어서지 못하게 그냥 단순히 잘라냄\n",
        "- RNN 에서 일반적으로 널리 사용됨\n",
        "- 적용\n",
        "  - 일반적인 최적화: `minimize()` 함수 사용 -> 그래디언트 계산과 적용 모두 처리\n",
        "  - 이를 `compute_gradients()` 와 `apply_gradients()` 로 분리하고, 그 사이에 `clip_by_value()` 함수를 적용하여 그래디언트를 클리핑함\n",
        "  \n"
      ]
    },
    {
      "metadata": {
        "id": "B1qrHZU8iqJv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 * 28  # MNIST\n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 50\n",
        "n_hidden3 = 50\n",
        "n_hidden4 = 50\n",
        "n_hidden5 = 50\n",
        "n_outputs = 10\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
        "\n",
        "with tf.name_scope(\"dnn\"):\n",
        "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
        "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
        "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
        "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
        "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
        "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
        "\n",
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nni85ITgiurg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "\n",
        "threshold = 1.0\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "# 그래디언트 계산\n",
        "grads_and_vars = optimizer.compute_gradients(loss)\n",
        "# 그래디언트 클리핑\n",
        "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
        "              for grad, var in grads_and_vars]\n",
        "# 그래디언트 적용\n",
        "training_op = optimizer.apply_gradients(capped_gvs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tm9g9TRji-od",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "나머지는 동일"
      ]
    },
    {
      "metadata": {
        "id": "FPQD9EQ7jAJE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "24a8357f-201f-46d0-ed92-32d44580453a"
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
        "    \n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "n_epochs = 20\n",
        "batch_size = 200\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "        print(epoch, \"검증 세트 정확도:\", accuracy_val)\n",
        "\n",
        "    save_path = saver.save(sess, \"./my_model_gradients_clipping_final.ckpt\")\n",
        "    \n",
        "! ls ./my_model_gradients_clipping_final*"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 검증 세트 정확도: 0.2876\n",
            "1 검증 세트 정확도: 0.7944\n",
            "2 검증 세트 정확도: 0.8794\n",
            "3 검증 세트 정확도: 0.9062\n",
            "4 검증 세트 정확도: 0.9162\n",
            "5 검증 세트 정확도: 0.9218\n",
            "6 검증 세트 정확도: 0.9296\n",
            "7 검증 세트 정확도: 0.9354\n",
            "8 검증 세트 정확도: 0.9382\n",
            "9 검증 세트 정확도: 0.9416\n",
            "10 검증 세트 정확도: 0.9456\n",
            "11 검증 세트 정확도: 0.947\n",
            "12 검증 세트 정확도: 0.9476\n",
            "13 검증 세트 정확도: 0.953\n",
            "14 검증 세트 정확도: 0.9564\n",
            "15 검증 세트 정확도: 0.9566\n",
            "16 검증 세트 정확도: 0.9578\n",
            "17 검증 세트 정확도: 0.9586\n",
            "18 검증 세트 정확도: 0.9626\n",
            "19 검증 세트 정확도: 0.9612\n",
            "./my_model_gradients_clipping_final.ckpt.data-00000-of-00001\n",
            "./my_model_gradients_clipping_final.ckpt.index\n",
            "./my_model_gradients_clipping_final.ckpt.meta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0NdKxcbojR5J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 11.2 미리 훈련된 층 재사용하기\n",
        "\n",
        "**Transfer learning**\n",
        "- 해결하려는 것과 비슷한 유형의 문제를 처리한 신경망이 이미 있는지 찾아보고 그런 다음 그 신경망의 하위층을 재사용하는 것이 좋음\n",
        "- 장점\n",
        "  - 훈련 속도를 크게 높여줌\n",
        "  - 필요한 훈련 데이터도 훨씬 적음\n",
        "- 원래 문제와 다른 크기의 이미지를 입력으로 사용한다면 원본 모델에 맞는 크기로 변경하는 전처리 필요함\n",
        "- 입력이 비슷한 저수준 특성을 가질 때 효과적\n",
        "  \n",
        "  ![fig_11_4](https://github.com/jonghoonseo/handson-ml/raw/wip/images/deep/fig_11_4.png)"
      ]
    },
    {
      "metadata": {
        "id": "mNWQXRDtlYyq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 11.2.1 텐서플로 모델 재사용하기\n",
        "\n",
        "먼저 그래프 구조를 로드해야 합니다. `import_meta_graph()` 함수가 그래프 연산들을 로드하여 기본 그래프에 적재하고 모델의 상태를 복원할 수 있도록 `Saver` 객체를 반환합니다. 기본적으로 `Saver` 객체는 `.meta` 확장자를 가진 파일에 그래프 구조를 저장하므로 이 파일을 로드해야 합니다:"
      ]
    },
    {
      "metadata": {
        "id": "1HzYzjIIlTaI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "ca61eecf-e7ac-4f07-e541-e6398015733e"
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "! ls *.meta\n",
        "saver = tf.train.import_meta_graph(\"./my_model_gradients_clipping_final.ckpt.meta\")"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_BN_model_final.ckpt.meta\n",
            "my_model_dropout_final.ckpt.meta\n",
            "my_model_final.ckpt.meta\n",
            "my_model_gradients_clipping_final.ckpt.meta\n",
            "my_model_max_norm2_final.ckpt.meta\n",
            "my_model_max_norm_final.ckpt.meta\n",
            "my_model_regularization2_final.ckpt.meta\n",
            "my_model_regulation_final.ckpt.meta\n",
            "my_new_model_caching_final.ckpt.meta\n",
            "my_new_model_final.ckpt.meta\n",
            "my_new_model_frozen_1_final.ckpt.meta\n",
            "my_new_model_frozen_2_final.ckpt.meta\n",
            "my_new_model_gradients_clipping_final_import_meta_graph.ckpt.meta\n",
            "my_new_model_gradients_clipping_final_Saver.ckpt.meta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wXJD3MPpmAo9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "다음으로 훈련해야 할 모든 연산을 가져와야 합니다. 그래프 구조를 모를 때는 모든 연산을 출력해 볼 수 있습니다:"
      ]
    },
    {
      "metadata": {
        "id": "eJoRbvZ0l__f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4763
        },
        "outputId": "826a646f-0db7-4e6f-8493-0b666362ee5e"
      },
      "cell_type": "code",
      "source": [
        "for op in tf.get_default_graph().get_operations():\n",
        "    print(op.name)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X\n",
            "y\n",
            "hidden1/kernel/Initializer/random_uniform/shape\n",
            "hidden1/kernel/Initializer/random_uniform/min\n",
            "hidden1/kernel/Initializer/random_uniform/max\n",
            "hidden1/kernel/Initializer/random_uniform/RandomUniform\n",
            "hidden1/kernel/Initializer/random_uniform/sub\n",
            "hidden1/kernel/Initializer/random_uniform/mul\n",
            "hidden1/kernel/Initializer/random_uniform\n",
            "hidden1/kernel\n",
            "hidden1/kernel/Assign\n",
            "hidden1/kernel/read\n",
            "hidden1/bias/Initializer/zeros\n",
            "hidden1/bias\n",
            "hidden1/bias/Assign\n",
            "hidden1/bias/read\n",
            "dnn/hidden1/MatMul\n",
            "dnn/hidden1/BiasAdd\n",
            "dnn/hidden1/Relu\n",
            "hidden2/kernel/Initializer/random_uniform/shape\n",
            "hidden2/kernel/Initializer/random_uniform/min\n",
            "hidden2/kernel/Initializer/random_uniform/max\n",
            "hidden2/kernel/Initializer/random_uniform/RandomUniform\n",
            "hidden2/kernel/Initializer/random_uniform/sub\n",
            "hidden2/kernel/Initializer/random_uniform/mul\n",
            "hidden2/kernel/Initializer/random_uniform\n",
            "hidden2/kernel\n",
            "hidden2/kernel/Assign\n",
            "hidden2/kernel/read\n",
            "hidden2/bias/Initializer/zeros\n",
            "hidden2/bias\n",
            "hidden2/bias/Assign\n",
            "hidden2/bias/read\n",
            "dnn/hidden2/MatMul\n",
            "dnn/hidden2/BiasAdd\n",
            "dnn/hidden2/Relu\n",
            "hidden3/kernel/Initializer/random_uniform/shape\n",
            "hidden3/kernel/Initializer/random_uniform/min\n",
            "hidden3/kernel/Initializer/random_uniform/max\n",
            "hidden3/kernel/Initializer/random_uniform/RandomUniform\n",
            "hidden3/kernel/Initializer/random_uniform/sub\n",
            "hidden3/kernel/Initializer/random_uniform/mul\n",
            "hidden3/kernel/Initializer/random_uniform\n",
            "hidden3/kernel\n",
            "hidden3/kernel/Assign\n",
            "hidden3/kernel/read\n",
            "hidden3/bias/Initializer/zeros\n",
            "hidden3/bias\n",
            "hidden3/bias/Assign\n",
            "hidden3/bias/read\n",
            "dnn/hidden3/MatMul\n",
            "dnn/hidden3/BiasAdd\n",
            "dnn/hidden3/Relu\n",
            "hidden4/kernel/Initializer/random_uniform/shape\n",
            "hidden4/kernel/Initializer/random_uniform/min\n",
            "hidden4/kernel/Initializer/random_uniform/max\n",
            "hidden4/kernel/Initializer/random_uniform/RandomUniform\n",
            "hidden4/kernel/Initializer/random_uniform/sub\n",
            "hidden4/kernel/Initializer/random_uniform/mul\n",
            "hidden4/kernel/Initializer/random_uniform\n",
            "hidden4/kernel\n",
            "hidden4/kernel/Assign\n",
            "hidden4/kernel/read\n",
            "hidden4/bias/Initializer/zeros\n",
            "hidden4/bias\n",
            "hidden4/bias/Assign\n",
            "hidden4/bias/read\n",
            "dnn/hidden4/MatMul\n",
            "dnn/hidden4/BiasAdd\n",
            "dnn/hidden4/Relu\n",
            "hidden5/kernel/Initializer/random_uniform/shape\n",
            "hidden5/kernel/Initializer/random_uniform/min\n",
            "hidden5/kernel/Initializer/random_uniform/max\n",
            "hidden5/kernel/Initializer/random_uniform/RandomUniform\n",
            "hidden5/kernel/Initializer/random_uniform/sub\n",
            "hidden5/kernel/Initializer/random_uniform/mul\n",
            "hidden5/kernel/Initializer/random_uniform\n",
            "hidden5/kernel\n",
            "hidden5/kernel/Assign\n",
            "hidden5/kernel/read\n",
            "hidden5/bias/Initializer/zeros\n",
            "hidden5/bias\n",
            "hidden5/bias/Assign\n",
            "hidden5/bias/read\n",
            "dnn/hidden5/MatMul\n",
            "dnn/hidden5/BiasAdd\n",
            "dnn/hidden5/Relu\n",
            "outputs/kernel/Initializer/random_uniform/shape\n",
            "outputs/kernel/Initializer/random_uniform/min\n",
            "outputs/kernel/Initializer/random_uniform/max\n",
            "outputs/kernel/Initializer/random_uniform/RandomUniform\n",
            "outputs/kernel/Initializer/random_uniform/sub\n",
            "outputs/kernel/Initializer/random_uniform/mul\n",
            "outputs/kernel/Initializer/random_uniform\n",
            "outputs/kernel\n",
            "outputs/kernel/Assign\n",
            "outputs/kernel/read\n",
            "outputs/bias/Initializer/zeros\n",
            "outputs/bias\n",
            "outputs/bias/Assign\n",
            "outputs/bias/read\n",
            "dnn/outputs/MatMul\n",
            "dnn/outputs/BiasAdd\n",
            "loss/SparseSoftmaxCrossEntropyWithLogits/Shape\n",
            "loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits\n",
            "loss/Const\n",
            "loss/loss\n",
            "gradients/Shape\n",
            "gradients/grad_ys_0\n",
            "gradients/Fill\n",
            "gradients/loss/loss_grad/Reshape/shape\n",
            "gradients/loss/loss_grad/Reshape\n",
            "gradients/loss/loss_grad/Shape\n",
            "gradients/loss/loss_grad/Tile\n",
            "gradients/loss/loss_grad/Shape_1\n",
            "gradients/loss/loss_grad/Shape_2\n",
            "gradients/loss/loss_grad/Const\n",
            "gradients/loss/loss_grad/Prod\n",
            "gradients/loss/loss_grad/Const_1\n",
            "gradients/loss/loss_grad/Prod_1\n",
            "gradients/loss/loss_grad/Maximum/y\n",
            "gradients/loss/loss_grad/Maximum\n",
            "gradients/loss/loss_grad/floordiv\n",
            "gradients/loss/loss_grad/Cast\n",
            "gradients/loss/loss_grad/truediv\n",
            "gradients/zeros_like\n",
            "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient\n",
            "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim\n",
            "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims\n",
            "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul\n",
            "gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad\n",
            "gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps\n",
            "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency\n",
            "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1\n",
            "gradients/dnn/outputs/MatMul_grad/MatMul\n",
            "gradients/dnn/outputs/MatMul_grad/MatMul_1\n",
            "gradients/dnn/outputs/MatMul_grad/tuple/group_deps\n",
            "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency\n",
            "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1\n",
            "gradients/dnn/hidden5/Relu_grad/ReluGrad\n",
            "gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad\n",
            "gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps\n",
            "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency\n",
            "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1\n",
            "gradients/dnn/hidden5/MatMul_grad/MatMul\n",
            "gradients/dnn/hidden5/MatMul_grad/MatMul_1\n",
            "gradients/dnn/hidden5/MatMul_grad/tuple/group_deps\n",
            "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency\n",
            "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1\n",
            "gradients/dnn/hidden4/Relu_grad/ReluGrad\n",
            "gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad\n",
            "gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps\n",
            "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency\n",
            "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1\n",
            "gradients/dnn/hidden4/MatMul_grad/MatMul\n",
            "gradients/dnn/hidden4/MatMul_grad/MatMul_1\n",
            "gradients/dnn/hidden4/MatMul_grad/tuple/group_deps\n",
            "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency\n",
            "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1\n",
            "gradients/dnn/hidden3/Relu_grad/ReluGrad\n",
            "gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad\n",
            "gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps\n",
            "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency\n",
            "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1\n",
            "gradients/dnn/hidden3/MatMul_grad/MatMul\n",
            "gradients/dnn/hidden3/MatMul_grad/MatMul_1\n",
            "gradients/dnn/hidden3/MatMul_grad/tuple/group_deps\n",
            "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency\n",
            "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1\n",
            "gradients/dnn/hidden2/Relu_grad/ReluGrad\n",
            "gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad\n",
            "gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps\n",
            "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency\n",
            "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1\n",
            "gradients/dnn/hidden2/MatMul_grad/MatMul\n",
            "gradients/dnn/hidden2/MatMul_grad/MatMul_1\n",
            "gradients/dnn/hidden2/MatMul_grad/tuple/group_deps\n",
            "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency\n",
            "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1\n",
            "gradients/dnn/hidden1/Relu_grad/ReluGrad\n",
            "gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad\n",
            "gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps\n",
            "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency\n",
            "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1\n",
            "gradients/dnn/hidden1/MatMul_grad/MatMul\n",
            "gradients/dnn/hidden1/MatMul_grad/MatMul_1\n",
            "gradients/dnn/hidden1/MatMul_grad/tuple/group_deps\n",
            "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency\n",
            "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1\n",
            "clip_by_value/Minimum/y\n",
            "clip_by_value/Minimum\n",
            "clip_by_value/y\n",
            "clip_by_value\n",
            "clip_by_value_1/Minimum/y\n",
            "clip_by_value_1/Minimum\n",
            "clip_by_value_1/y\n",
            "clip_by_value_1\n",
            "clip_by_value_2/Minimum/y\n",
            "clip_by_value_2/Minimum\n",
            "clip_by_value_2/y\n",
            "clip_by_value_2\n",
            "clip_by_value_3/Minimum/y\n",
            "clip_by_value_3/Minimum\n",
            "clip_by_value_3/y\n",
            "clip_by_value_3\n",
            "clip_by_value_4/Minimum/y\n",
            "clip_by_value_4/Minimum\n",
            "clip_by_value_4/y\n",
            "clip_by_value_4\n",
            "clip_by_value_5/Minimum/y\n",
            "clip_by_value_5/Minimum\n",
            "clip_by_value_5/y\n",
            "clip_by_value_5\n",
            "clip_by_value_6/Minimum/y\n",
            "clip_by_value_6/Minimum\n",
            "clip_by_value_6/y\n",
            "clip_by_value_6\n",
            "clip_by_value_7/Minimum/y\n",
            "clip_by_value_7/Minimum\n",
            "clip_by_value_7/y\n",
            "clip_by_value_7\n",
            "clip_by_value_8/Minimum/y\n",
            "clip_by_value_8/Minimum\n",
            "clip_by_value_8/y\n",
            "clip_by_value_8\n",
            "clip_by_value_9/Minimum/y\n",
            "clip_by_value_9/Minimum\n",
            "clip_by_value_9/y\n",
            "clip_by_value_9\n",
            "clip_by_value_10/Minimum/y\n",
            "clip_by_value_10/Minimum\n",
            "clip_by_value_10/y\n",
            "clip_by_value_10\n",
            "clip_by_value_11/Minimum/y\n",
            "clip_by_value_11/Minimum\n",
            "clip_by_value_11/y\n",
            "clip_by_value_11\n",
            "GradientDescent/learning_rate\n",
            "GradientDescent/update_hidden1/kernel/ApplyGradientDescent\n",
            "GradientDescent/update_hidden1/bias/ApplyGradientDescent\n",
            "GradientDescent/update_hidden2/kernel/ApplyGradientDescent\n",
            "GradientDescent/update_hidden2/bias/ApplyGradientDescent\n",
            "GradientDescent/update_hidden3/kernel/ApplyGradientDescent\n",
            "GradientDescent/update_hidden3/bias/ApplyGradientDescent\n",
            "GradientDescent/update_hidden4/kernel/ApplyGradientDescent\n",
            "GradientDescent/update_hidden4/bias/ApplyGradientDescent\n",
            "GradientDescent/update_hidden5/kernel/ApplyGradientDescent\n",
            "GradientDescent/update_hidden5/bias/ApplyGradientDescent\n",
            "GradientDescent/update_outputs/kernel/ApplyGradientDescent\n",
            "GradientDescent/update_outputs/bias/ApplyGradientDescent\n",
            "GradientDescent\n",
            "eval/in_top_k/InTopKV2/k\n",
            "eval/in_top_k/InTopKV2\n",
            "eval/Cast\n",
            "eval/Const\n",
            "eval/accuracy\n",
            "init\n",
            "save/Const\n",
            "save/SaveV2/tensor_names\n",
            "save/SaveV2/shape_and_slices\n",
            "save/SaveV2\n",
            "save/control_dependency\n",
            "save/RestoreV2/tensor_names\n",
            "save/RestoreV2/shape_and_slices\n",
            "save/RestoreV2\n",
            "save/Assign\n",
            "save/Assign_1\n",
            "save/Assign_2\n",
            "save/Assign_3\n",
            "save/Assign_4\n",
            "save/Assign_5\n",
            "save/Assign_6\n",
            "save/Assign_7\n",
            "save/Assign_8\n",
            "save/Assign_9\n",
            "save/Assign_10\n",
            "save/Assign_11\n",
            "save/restore_all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v-YTKIndmWFJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "너무 많으면 tensorboard 이용"
      ]
    },
    {
      "metadata": {
        "id": "pzTHUIhAmZM_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1749
        },
        "outputId": "12d717d8-db5c-4669-8d62-b46d0da37006"
      },
      "cell_type": "code",
      "source": [
        "! rm tensorflow_graph_in_jupyter.py* wget-log*\n",
        "! wget https://github.com/rickiepark/handson-ml/raw/master/tensorflow_graph_in_jupyter.py\n",
        "! ls\n",
        "\n",
        "from tensorflow_graph_in_jupyter import show_graph\n",
        "\n",
        "show_graph(tf.get_default_graph())"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'wget-log*': No such file or directory\n",
            "--2018-10-12 16:07:20--  https://github.com/rickiepark/handson-ml/raw/master/tensorflow_graph_in_jupyter.py\n",
            "Resolving github.com (github.com)... 192.30.253.112, 192.30.253.113\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/rickiepark/handson-ml/master/tensorflow_graph_in_jupyter.py [following]\n",
            "--2018-10-12 16:07:20--  https://raw.githubusercontent.com/rickiepark/handson-ml/master/tensorflow_graph_in_jupyter.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2159 (2.1K) [text/plain]\n",
            "Saving to: ‘tensorflow_graph_in_jupyter.py’\n",
            "\n",
            "tensorflow_graph_in 100%[===================>]   2.11K  --.-KB/s    in 0s      \n",
            "\n",
            "2018-10-12 16:07:21 (33.0 MB/s) - ‘tensorflow_graph_in_jupyter.py’ saved [2159/2159]\n",
            "\n",
            "checkpoint\n",
            "images\n",
            "my_BN_model_final.ckpt.data-00000-of-00001\n",
            "my_BN_model_final.ckpt.index\n",
            "my_BN_model_final.ckpt.meta\n",
            "my_model_dropout_final.ckpt.data-00000-of-00001\n",
            "my_model_dropout_final.ckpt.index\n",
            "my_model_dropout_final.ckpt.meta\n",
            "my_model_final.ckpt.data-00000-of-00001\n",
            "my_model_final.ckpt.index\n",
            "my_model_final.ckpt.meta\n",
            "my_model_gradients_clipping_final.ckpt.data-00000-of-00001\n",
            "my_model_gradients_clipping_final.ckpt.index\n",
            "my_model_gradients_clipping_final.ckpt.meta\n",
            "my_model_max_norm2_final.ckpt.data-00000-of-00001\n",
            "my_model_max_norm2_final.ckpt.index\n",
            "my_model_max_norm2_final.ckpt.meta\n",
            "my_model_max_norm_final.ckpt.data-00000-of-00001\n",
            "my_model_max_norm_final.ckpt.index\n",
            "my_model_max_norm_final.ckpt.meta\n",
            "my_model_regularization2_final.ckpt.data-00000-of-00001\n",
            "my_model_regularization2_final.ckpt.index\n",
            "my_model_regularization2_final.ckpt.meta\n",
            "my_model_regulation_final.ckpt.data-00000-of-00001\n",
            "my_model_regulation_final.ckpt.index\n",
            "my_model_regulation_final.ckpt.meta\n",
            "my_new_model_caching_final.ckpt.data-00000-of-00001\n",
            "my_new_model_caching_final.ckpt.index\n",
            "my_new_model_caching_final.ckpt.meta\n",
            "my_new_model_final.ckpt.data-00000-of-00001\n",
            "my_new_model_final.ckpt.index\n",
            "my_new_model_final.ckpt.meta\n",
            "my_new_model_frozen_1_final.ckpt.data-00000-of-00001\n",
            "my_new_model_frozen_1_final.ckpt.index\n",
            "my_new_model_frozen_1_final.ckpt.meta\n",
            "my_new_model_frozen_2_final.ckpt.data-00000-of-00001\n",
            "my_new_model_frozen_2_final.ckpt.index\n",
            "my_new_model_frozen_2_final.ckpt.meta\n",
            "my_new_model_gradients_clipping_final_import_meta_graph.ckpt.data-00000-of-00001\n",
            "my_new_model_gradients_clipping_final_import_meta_graph.ckpt.index\n",
            "my_new_model_gradients_clipping_final_import_meta_graph.ckpt.meta\n",
            "my_new_model_gradients_clipping_final_Saver.ckpt.data-00000-of-00001\n",
            "my_new_model_gradients_clipping_final_Saver.ckpt.index\n",
            "my_new_model_gradients_clipping_final_Saver.ckpt.meta\n",
            "__pycache__\n",
            "sample_data\n",
            "tensorflow_graph_in_jupyter.py\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
              "        <script src=&quot;//cdnjs.cloudflare.com/ajax/libs/polymer/0.3.3/platform.js&quot;></script>\n",
              "        <script>\n",
              "          function load() {\n",
              "            document.getElementById(&quot;graph0.3745401188473625&quot;).pbtxt = 'node {\\n  name: &quot;X&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: -1\\n        }\\n        dim {\\n          size: 784\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;y&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        unknown_rank: true\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\020\\\\003\\\\000\\\\000,\\\\001\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.07439795136451721\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.07439795136451721\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 5\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 784\\n        }\\n        dim {\\n          size: 300\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 300\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 300\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;hidden1/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden1/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden1/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;hidden1/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden1/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden1/MatMul&quot;\\n  input: &quot;hidden1/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden1/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden1/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;,\\\\001\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.13093073666095734\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.13093073666095734\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 22\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 300\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;hidden2/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden2/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden2/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden1/Relu&quot;\\n  input: &quot;hidden2/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden2/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden2/MatMul&quot;\\n  input: &quot;hidden2/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden2/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden2/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 39\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;hidden3/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden3/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden3/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden2/Relu&quot;\\n  input: &quot;hidden3/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden3/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden3/MatMul&quot;\\n  input: &quot;hidden3/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden3/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden3/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 56\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;hidden4/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden4/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden4/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden3/Relu&quot;\\n  input: &quot;hidden4/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden4/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden4/MatMul&quot;\\n  input: &quot;hidden4/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden4/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden4/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 73\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden5/bias&quot;\\n  input: &quot;hidden5/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden5/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden5/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden4/Relu&quot;\\n  input: &quot;hidden5/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden5/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden5/MatMul&quot;\\n  input: &quot;hidden5/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden5/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden5/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\000\\\\n\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.3162277638912201\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.3162277638912201\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 90\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 10\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/kernel&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;outputs/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 10\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 10\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;outputs/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;outputs/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/outputs/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden5/Relu&quot;\\n  input: &quot;outputs/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/outputs/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/outputs/MatMul&quot;\\n  input: &quot;outputs/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  op: &quot;SparseSoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;dnn/outputs/BiasAdd&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tlabels&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/loss&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;loss/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/grad_ys_0&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Shape&quot;\\n  input: &quot;gradients/grad_ys_0&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;index_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Fill&quot;\\n  input: &quot;gradients/loss/loss_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/loss/loss_grad/Reshape&quot;\\n  input: &quot;gradients/loss/loss_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Shape_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Prod&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/loss/loss_grad/Shape_1&quot;\\n  input: &quot;gradients/loss/loss_grad/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Const_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Prod_1&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/loss/loss_grad/Shape_2&quot;\\n  input: &quot;gradients/loss/loss_grad/Const_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/loss/loss_grad/Prod_1&quot;\\n  input: &quot;gradients/loss/loss_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/loss/loss_grad/Prod&quot;\\n  input: &quot;gradients/loss/loss_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;gradients/loss/loss_grad/floordiv&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Truncate&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/truediv&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients/loss/loss_grad/Tile&quot;\\n  input: &quot;gradients/loss/loss_grad/Cast&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/zeros_like&quot;\\n  op: &quot;ZerosLike&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient&quot;\\n  op: &quot;PreventGradient&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;message&quot;\\n    value {\\n      s: &quot;Currently there is no way to take the second derivative of sparse_softmax_cross_entropy_with_logits due to the fused implementation\\\\\\'s interaction with tf.gradients()&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: -1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims&quot;\\n  op: &quot;ExpandDims&quot;\\n  input: &quot;gradients/loss/loss_grad/truediv&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tdim&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  input: &quot;^gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;outputs/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden5/Relu&quot;\\n  input: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/outputs/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden5/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden5/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden4/Relu&quot;\\n  input: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden5/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden5/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden5/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden5/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden5/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden4/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden4/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden3/Relu&quot;\\n  input: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden4/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden4/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden3/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden3/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden2/Relu&quot;\\n  input: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden3/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden3/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden2/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden2/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden1/Relu&quot;\\n  input: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden2/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden2/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden1/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden1/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden1/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden1/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value/Minimum&quot;\\n  input: &quot;clip_by_value/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_1/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_1/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_1/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_1/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_1&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_1/Minimum&quot;\\n  input: &quot;clip_by_value_1/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_2/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_2/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_2/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_2/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_2&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_2/Minimum&quot;\\n  input: &quot;clip_by_value_2/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_3/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_3/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_3/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_3/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_3&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_3/Minimum&quot;\\n  input: &quot;clip_by_value_3/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_4/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_4/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_4/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_4/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_4&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_4/Minimum&quot;\\n  input: &quot;clip_by_value_4/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_5/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_5/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_5/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_5/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_5&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_5/Minimum&quot;\\n  input: &quot;clip_by_value_5/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_6/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_6/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_6/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_6/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_6&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_6/Minimum&quot;\\n  input: &quot;clip_by_value_6/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_7/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_7/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_7/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_7/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_7&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_7/Minimum&quot;\\n  input: &quot;clip_by_value_7/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_8/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_8/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_8/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_8/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_8&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_8/Minimum&quot;\\n  input: &quot;clip_by_value_8/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_9/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_9/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_9/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_9/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_9&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_9/Minimum&quot;\\n  input: &quot;clip_by_value_9/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_10/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_10/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_10/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_10/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_10&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_10/Minimum&quot;\\n  input: &quot;clip_by_value_10/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_11/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_11/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_11/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_11/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_11&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_11/Minimum&quot;\\n  input: &quot;clip_by_value_11/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/learning_rate&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.009999999776482582\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden1/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden1/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden2/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden2/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden3/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_4&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden3/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_5&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden4/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_6&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden4/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_7&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden5/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_8&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden5/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden5/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_9&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_outputs/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;outputs/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_10&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_outputs/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_11&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^GradientDescent/update_hidden1/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden1/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden2/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden2/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden3/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden3/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden4/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden4/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden5/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden5/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_outputs/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_outputs/kernel/ApplyGradientDescent&quot;\\n}\\nnode {\\n  name: &quot;eval/in_top_k/InTopKV2/k&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/in_top_k/InTopKV2&quot;\\n  op: &quot;InTopKV2&quot;\\n  input: &quot;dnn/outputs/BiasAdd&quot;\\n  input: &quot;y&quot;\\n  input: &quot;eval/in_top_k/InTopKV2/k&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;eval/in_top_k/InTopKV2&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_BOOL\\n    }\\n  }\\n  attr {\\n    key: &quot;Truncate&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/accuracy&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;eval/Cast&quot;\\n  input: &quot;eval/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^hidden1/bias/Assign&quot;\\n  input: &quot;^hidden1/kernel/Assign&quot;\\n  input: &quot;^hidden2/bias/Assign&quot;\\n  input: &quot;^hidden2/kernel/Assign&quot;\\n  input: &quot;^hidden3/bias/Assign&quot;\\n  input: &quot;^hidden3/kernel/Assign&quot;\\n  input: &quot;^hidden4/bias/Assign&quot;\\n  input: &quot;^hidden4/kernel/Assign&quot;\\n  input: &quot;^hidden5/bias/Assign&quot;\\n  input: &quot;^hidden5/kernel/Assign&quot;\\n  input: &quot;^outputs/bias/Assign&quot;\\n  input: &quot;^outputs/kernel/Assign&quot;\\n}\\nnode {\\n  name: &quot;save/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n        }\\n        string_val: &quot;model&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 12\\n          }\\n        }\\n        string_val: &quot;hidden1/bias&quot;\\n        string_val: &quot;hidden1/kernel&quot;\\n        string_val: &quot;hidden2/bias&quot;\\n        string_val: &quot;hidden2/kernel&quot;\\n        string_val: &quot;hidden3/bias&quot;\\n        string_val: &quot;hidden3/kernel&quot;\\n        string_val: &quot;hidden4/bias&quot;\\n        string_val: &quot;hidden4/kernel&quot;\\n        string_val: &quot;hidden5/bias&quot;\\n        string_val: &quot;hidden5/kernel&quot;\\n        string_val: &quot;outputs/bias&quot;\\n        string_val: &quot;outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 12\\n          }\\n        }\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2&quot;\\n  op: &quot;SaveV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/SaveV2/tensor_names&quot;\\n  input: &quot;save/SaveV2/shape_and_slices&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;hidden5/bias&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;outputs/kernel&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;^save/SaveV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@save/Const&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 12\\n          }\\n        }\\n        string_val: &quot;hidden1/bias&quot;\\n        string_val: &quot;hidden1/kernel&quot;\\n        string_val: &quot;hidden2/bias&quot;\\n        string_val: &quot;hidden2/kernel&quot;\\n        string_val: &quot;hidden3/bias&quot;\\n        string_val: &quot;hidden3/kernel&quot;\\n        string_val: &quot;hidden4/bias&quot;\\n        string_val: &quot;hidden4/kernel&quot;\\n        string_val: &quot;hidden5/bias&quot;\\n        string_val: &quot;hidden5/kernel&quot;\\n        string_val: &quot;outputs/bias&quot;\\n        string_val: &quot;outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 12\\n          }\\n        }\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2/tensor_names&quot;\\n  input: &quot;save/RestoreV2/shape_and_slices&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;save/RestoreV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_1&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;save/RestoreV2:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_2&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;save/RestoreV2:2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_3&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;save/RestoreV2:3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_4&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;save/RestoreV2:4&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_5&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;save/RestoreV2:5&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_6&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;save/RestoreV2:6&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_7&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;save/RestoreV2:7&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_8&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden5/bias&quot;\\n  input: &quot;save/RestoreV2:8&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_9&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  input: &quot;save/RestoreV2:9&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_10&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;save/RestoreV2:10&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_11&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/kernel&quot;\\n  input: &quot;save/RestoreV2:11&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/restore_all&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^save/Assign&quot;\\n  input: &quot;^save/Assign_1&quot;\\n  input: &quot;^save/Assign_10&quot;\\n  input: &quot;^save/Assign_11&quot;\\n  input: &quot;^save/Assign_2&quot;\\n  input: &quot;^save/Assign_3&quot;\\n  input: &quot;^save/Assign_4&quot;\\n  input: &quot;^save/Assign_5&quot;\\n  input: &quot;^save/Assign_6&quot;\\n  input: &quot;^save/Assign_7&quot;\\n  input: &quot;^save/Assign_8&quot;\\n  input: &quot;^save/Assign_9&quot;\\n}\\n';\n",
              "          }\n",
              "        </script>\n",
              "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
              "        <div style=&quot;height:600px&quot;>\n",
              "          <tf-graph-basic id=&quot;graph0.3745401188473625&quot;></tf-graph-basic>\n",
              "        </div>\n",
              "    \"></iframe>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "kav1kxZknnnf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "필요한 연산을 찾았다면 그래프의 `get_operation_by_name()`이나 `get_tensor_by_name()` 메서드를 사용하여 추출할 수 있습니다:"
      ]
    },
    {
      "metadata": {
        "id": "4lLUDpfgnSJX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
        "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
        "\n",
        "accuracy = tf.get_default_graph().get_tensor_by_name(\"eval/accuracy:0\")\n",
        "\n",
        "training_op = tf.get_default_graph().get_operation_by_name(\"GradientDescent\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GSfwvz4tpfSL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "재사용성 높이기\n",
        "1. 연산에 명확한 이름 부여\n",
        "2. 문서화\n",
        "3. **중요한 연산들을 모아놓은 컬렉션 만들기**"
      ]
    },
    {
      "metadata": {
        "id": "5F7dr3n9prV8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for op in (X, y, accuracy, training_op):\n",
        "    tf.add_to_collection(\"my_important_ops\", op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rvksp8IJpw2w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "컬렉션 재사용 시"
      ]
    },
    {
      "metadata": {
        "id": "pMh_laV-pyEn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X, y, accuracy, training_op = tf.get_collection(\"my_important_ops\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7ICKjrpMp5VQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**1. 세션을 시작**하고 **2. 모델을 복원**하여 **3. 준비된 훈련 데이터로 훈련을 계속**할 수 있습니다:"
      ]
    },
    {
      "metadata": {
        "id": "a0MhEbeEqH6u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1173
        },
        "outputId": "2671a616-719c-4e4e-b03e-11cec8e789dc"
      },
      "cell_type": "code",
      "source": [
        "! ls\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    saver.restore(sess, \"my_model_gradients_clipping_final.ckpt\")\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "        print(epoch, \"검증 세트 정확도:\", accuracy_val)\n",
        "\n",
        "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint\n",
            "images\n",
            "my_BN_model_final.ckpt.data-00000-of-00001\n",
            "my_BN_model_final.ckpt.index\n",
            "my_BN_model_final.ckpt.meta\n",
            "my_model_dropout_final.ckpt.data-00000-of-00001\n",
            "my_model_dropout_final.ckpt.index\n",
            "my_model_dropout_final.ckpt.meta\n",
            "my_model_final.ckpt.data-00000-of-00001\n",
            "my_model_final.ckpt.index\n",
            "my_model_final.ckpt.meta\n",
            "my_model_gradients_clipping_final.ckpt.data-00000-of-00001\n",
            "my_model_gradients_clipping_final.ckpt.index\n",
            "my_model_gradients_clipping_final.ckpt.meta\n",
            "my_model_max_norm2_final.ckpt.data-00000-of-00001\n",
            "my_model_max_norm2_final.ckpt.index\n",
            "my_model_max_norm2_final.ckpt.meta\n",
            "my_model_max_norm_final.ckpt.data-00000-of-00001\n",
            "my_model_max_norm_final.ckpt.index\n",
            "my_model_max_norm_final.ckpt.meta\n",
            "my_model_regularization2_final.ckpt.data-00000-of-00001\n",
            "my_model_regularization2_final.ckpt.index\n",
            "my_model_regularization2_final.ckpt.meta\n",
            "my_model_regulation_final.ckpt.data-00000-of-00001\n",
            "my_model_regulation_final.ckpt.index\n",
            "my_model_regulation_final.ckpt.meta\n",
            "my_new_model_caching_final.ckpt.data-00000-of-00001\n",
            "my_new_model_caching_final.ckpt.index\n",
            "my_new_model_caching_final.ckpt.meta\n",
            "my_new_model_final.ckpt.data-00000-of-00001\n",
            "my_new_model_final.ckpt.index\n",
            "my_new_model_final.ckpt.meta\n",
            "my_new_model_frozen_1_final.ckpt.data-00000-of-00001\n",
            "my_new_model_frozen_1_final.ckpt.index\n",
            "my_new_model_frozen_1_final.ckpt.meta\n",
            "my_new_model_frozen_2_final.ckpt.data-00000-of-00001\n",
            "my_new_model_frozen_2_final.ckpt.index\n",
            "my_new_model_frozen_2_final.ckpt.meta\n",
            "my_new_model_gradients_clipping_final_import_meta_graph.ckpt.data-00000-of-00001\n",
            "my_new_model_gradients_clipping_final_import_meta_graph.ckpt.index\n",
            "my_new_model_gradients_clipping_final_import_meta_graph.ckpt.meta\n",
            "my_new_model_gradients_clipping_final_Saver.ckpt.data-00000-of-00001\n",
            "my_new_model_gradients_clipping_final_Saver.ckpt.index\n",
            "my_new_model_gradients_clipping_final_Saver.ckpt.meta\n",
            "__pycache__\n",
            "sample_data\n",
            "tensorflow_graph_in_jupyter.py\n",
            "INFO:tensorflow:Restoring parameters from my_model_gradients_clipping_final.ckpt\n",
            "0 검증 세트 정확도: 0.9634\n",
            "1 검증 세트 정확도: 0.9628\n",
            "2 검증 세트 정확도: 0.9652\n",
            "3 검증 세트 정확도: 0.9652\n",
            "4 검증 세트 정확도: 0.9642\n",
            "5 검증 세트 정확도: 0.965\n",
            "6 검증 세트 정확도: 0.9686\n",
            "7 검증 세트 정확도: 0.9686\n",
            "8 검증 세트 정확도: 0.9684\n",
            "9 검증 세트 정확도: 0.9686\n",
            "10 검증 세트 정확도: 0.9706\n",
            "11 검증 세트 정확도: 0.9714\n",
            "12 검증 세트 정확도: 0.9672\n",
            "13 검증 세트 정확도: 0.9702\n",
            "14 검증 세트 정확도: 0.9708\n",
            "15 검증 세트 정확도: 0.9724\n",
            "16 검증 세트 정확도: 0.9722\n",
            "17 검증 세트 정확도: 0.9712\n",
            "18 검증 세트 정확도: 0.9712\n",
            "19 검증 세트 정확도: 0.9712\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZGFuQr8fsrKu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "다른 방법:\n",
        "- 원본 그래프를 만든 파이썬 코드에 접근 가능할 때 `import_meta_graph()` 사용"
      ]
    },
    {
      "metadata": {
        "id": "mSxn3m5Vsqim",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 원본 그래프를 만든 파이썬 코드\n",
        "\n",
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 * 28  # MNIST\n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 50\n",
        "n_hidden3 = 50\n",
        "n_hidden4 = 50\n",
        "n_outputs = 10\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
        "\n",
        "with tf.name_scope(\"dnn\"):\n",
        "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
        "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
        "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
        "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
        "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
        "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
        "\n",
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
        "\n",
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
        "\n",
        "learning_rate = 0.01\n",
        "threshold = 1.0\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "grads_and_vars = optimizer.compute_gradients(loss)\n",
        "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
        "              for grad, var in grads_and_vars]\n",
        "training_op = optimizer.apply_gradients(capped_gvs)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aFtDUIxxtUn5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "6c37a288-858c-41c4-8bae-7bd4ea119368"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    saver.restore(sess, \"./my_model_gradients_clipping_final.ckpt\")\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "        print(epoch, \"검증 세트 정확도:\", accuracy_val)\n",
        "\n",
        "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")    "
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./my_model_gradients_clipping_final.ckpt\n",
            "0 검증 세트 정확도: 0.9636\n",
            "1 검증 세트 정확도: 0.9624\n",
            "2 검증 세트 정확도: 0.9652\n",
            "3 검증 세트 정확도: 0.9654\n",
            "4 검증 세트 정확도: 0.9642\n",
            "5 검증 세트 정확도: 0.965\n",
            "6 검증 세트 정확도: 0.9686\n",
            "7 검증 세트 정확도: 0.9682\n",
            "8 검증 세트 정확도: 0.968\n",
            "9 검증 세트 정확도: 0.9686\n",
            "10 검증 세트 정확도: 0.9704\n",
            "11 검증 세트 정확도: 0.9716\n",
            "12 검증 세트 정확도: 0.9672\n",
            "13 검증 세트 정확도: 0.9702\n",
            "14 검증 세트 정확도: 0.9708\n",
            "15 검증 세트 정확도: 0.9724\n",
            "16 검증 세트 정확도: 0.9718\n",
            "17 검증 세트 정확도: 0.971\n",
            "18 검증 세트 정확도: 0.9712\n",
            "19 검증 세트 정확도: 0.9712\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vFuvn-5qvX7M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "하위층만 재사용\n",
        "\n",
        "- `import_meta_graph()`를 사용하면 전체 그래프를 로드하지만 필요하지 않은 부분은 무시\n",
        "- 이 예에서는 학습된 3번째 층 위에 4번째 은닉층을 새로 추가(원래 4번째 층은 무시)\n"
      ]
    },
    {
      "metadata": {
        "id": "0NjjWF0FvqRa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "n_hidden4 = 20  # 새 층\n",
        "n_outputs = 10  # 새 층\n",
        "\n",
        "# import_meta_graph() 사용한 복원 - 전체 그래프 로드\n",
        "saver = tf.train.import_meta_graph(\"./my_model_gradients_clipping_final.ckpt.meta\")\n",
        "\n",
        "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
        "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
        "\n",
        "hidden3 = tf.get_default_graph().get_tensor_by_name(\"dnn/hidden3/Relu:0\") # 기존 층\n",
        "\n",
        "# 새 층\n",
        "new_hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"new_hidden4\")\n",
        "new_logits = tf.layers.dense(new_hidden4, n_outputs, name=\"new_outputs\")\n",
        "\n",
        "with tf.name_scope(\"new_loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=new_logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
        "\n",
        "with tf.name_scope(\"new_eval\"):\n",
        "    correct = tf.nn.in_top_k(new_logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
        "\n",
        "with tf.name_scope(\"new_train\"):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    training_op = optimizer.minimize(loss)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "new_saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YJtH_iXZwGy9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "새로운 모델을 훈련"
      ]
    },
    {
      "metadata": {
        "id": "z8Wg893gwIay",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "42dae716-27b5-4c8a-b6fe-1bd77be39a93"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    saver.restore(sess, \"./my_model_gradients_clipping_final.ckpt\")\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "        print(epoch, \"검증 세트 정확도:\", accuracy_val)\n",
        "\n",
        "    save_path = new_saver.save(sess, \"./my_new_model_gradients_clipping_final_import_meta_graph.ckpt\")"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./my_model_gradients_clipping_final.ckpt\n",
            "0 검증 세트 정확도: 0.9122\n",
            "1 검증 세트 정확도: 0.9376\n",
            "2 검증 세트 정확도: 0.946\n",
            "3 검증 세트 정확도: 0.9502\n",
            "4 검증 세트 정확도: 0.953\n",
            "5 검증 세트 정확도: 0.9524\n",
            "6 검증 세트 정확도: 0.9566\n",
            "7 검증 세트 정확도: 0.96\n",
            "8 검증 세트 정확도: 0.9616\n",
            "9 검증 세트 정확도: 0.961\n",
            "10 검증 세트 정확도: 0.9632\n",
            "11 검증 세트 정확도: 0.9626\n",
            "12 검증 세트 정확도: 0.9648\n",
            "13 검증 세트 정확도: 0.9656\n",
            "14 검증 세트 정확도: 0.9662\n",
            "15 검증 세트 정확도: 0.967\n",
            "16 검증 세트 정확도: 0.9678\n",
            "17 검증 세트 정확도: 0.9676\n",
            "18 검증 세트 정확도: 0.9684\n",
            "19 검증 세트 정확도: 0.9678\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EkpqiaAgy2mc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "원본 모델 코드에 접근 가능하다면 필요한 부분만 재사용 가능"
      ]
    },
    {
      "metadata": {
        "id": "DzjXpiPCy8nn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 * 28  # MNIST\n",
        "n_hidden1 = 300 # 재사용\n",
        "n_hidden2 = 50  # 재사용\n",
        "n_hidden3 = 50  # 재사용\n",
        "n_hidden4 = 20  # 새로 만듦!\n",
        "n_outputs = 10  # 새로 만듦!\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
        "\n",
        "with tf.name_scope(\"dnn\"):\n",
        "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")       # 재사용\n",
        "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # 재사용\n",
        "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # 재사용\n",
        "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # 새로 만듦!\n",
        "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")                         # 새로 만듦!\n",
        "\n",
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
        "\n",
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
        "\n",
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    training_op = optimizer.minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UZ7afYm22XQX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "2개의 `Saver()` 객체 생성 - 하나는 복원, 하나는 저장을 위한 객체\n",
        "\n",
        "복원\n",
        "- 가져오고자 하는 층을 `get_collection()` 에서 `scope`을 정규표현식을 이용하여 정의하고\n",
        "- `tf.train.Saver()`로 복원"
      ]
    },
    {
      "metadata": {
        "id": "0ob1s-_q0rz3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "bef2dd0e-1bf6-4613-e036-e826661f0979"
      },
      "cell_type": "code",
      "source": [
        "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
        "                               scope=\"hidden[123]\") # 정규표현식\n",
        "# 1~3층 복원을 위한 Saver() 객체\n",
        "restore_saver = tf.train.Saver(reuse_vars) # 1-3층 복원\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "# 저장을 위한 Saver() 객체\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    restore_saver.restore(sess, \"./my_model_gradients_clipping_final.ckpt\")\n",
        "\n",
        "    for epoch in range(n_epochs):                                        # 책에는 없음\n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size): # 책에는 없음\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})    # 책에는 없음\n",
        "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid}) # 책에는 없음\n",
        "        print(epoch, \"검증 세트 정확도:\", accuracy_val)                      # 책에는 없음\n",
        "\n",
        "    save_path = saver.save(sess, \"./my_new_model_gradients_clipping_final_Saver.ckpt\")"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./my_model_gradients_clipping_final.ckpt\n",
            "0 검증 세트 정확도: 0.9018\n",
            "1 검증 세트 정확도: 0.933\n",
            "2 검증 세트 정확도: 0.9428\n",
            "3 검증 세트 정확도: 0.947\n",
            "4 검증 세트 정확도: 0.9518\n",
            "5 검증 세트 정확도: 0.9536\n",
            "6 검증 세트 정확도: 0.9556\n",
            "7 검증 세트 정확도: 0.959\n",
            "8 검증 세트 정확도: 0.9586\n",
            "9 검증 세트 정확도: 0.9606\n",
            "10 검증 세트 정확도: 0.9624\n",
            "11 검증 세트 정확도: 0.962\n",
            "12 검증 세트 정확도: 0.9642\n",
            "13 검증 세트 정확도: 0.9664\n",
            "14 검증 세트 정확도: 0.9662\n",
            "15 검증 세트 정확도: 0.9664\n",
            "16 검증 세트 정확도: 0.967\n",
            "17 검증 세트 정확도: 0.9674\n",
            "18 검증 세트 정확도: 0.968\n",
            "19 검증 세트 정확도: 0.9674\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pjFuAUaD3Q4A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 11.2.2 다른 프레임워크의 모델 재사용하기\n",
        "\n",
        "만약 모델이 다른 프레임워크로 훈련되어 있다면 수동으로 모델 파라미터를 읽어 적절한 변수에 할당해야 함.\n",
        "\n",
        "모든 텐서플로 변수(`v`)는 아래 3개의 연산 노드를 가짐\n",
        "- 값을 읽는 연산: `v/read`\n",
        "- 값을 초기화 하는 연산: `v/initial_value`\n",
        "- 값을 할당하는 연산: `v/Assign`\n",
        "  - 변수 `v`의 할당 연산 `v/Assign`의 첫번째 입력은 변수 자체(`v`)고,<br/>\n",
        "    두번째 연산은 변수에 할당될 값(`v/initial_value`)임.\n",
        "  - 할당 연산의 두번째 입력(`[1]`)에 초깃값을 주입\n"
      ]
    },
    {
      "metadata": {
        "id": "-bpBO26C37PA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d702e0b2-427d-4ee6-9481-b5168a2cd6df"
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 2\n",
        "n_hidden1 = 3\n",
        "\n",
        "original_w = [[1., 2., 3.], [4., 5., 6.]] # 다른 프레임워크로부터 가중치를 로드\n",
        "original_b = [7., 8., 9.]                 # 다른 프레임워크로부터 편향을 로드\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
        "# [...] 모델의 나머지 부분을 구성\n",
        "\n",
        "# hidden1 변수의 할당 노드에 대한 핸들을 구합니다\n",
        "graph = tf.get_default_graph()\n",
        "assign_kernel = graph.get_operation_by_name(\"hidden1/kernel/Assign\")\n",
        "assign_bias = graph.get_operation_by_name(\"hidden1/bias/Assign\")\n",
        "### 할당 연산의 두번째 입력\n",
        "init_kernel = assign_kernel.inputs[1]\n",
        "init_bias = assign_bias.inputs[1]\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    ### feed_dict 를 이용하여 새로운 값을 주입함\n",
        "    sess.run(init, feed_dict={init_kernel: original_w, init_bias: original_b})\n",
        "    # [...] 새 작업에 모델을 훈련시킵니다\n",
        "    print(hidden1.eval(feed_dict={X: [[10.0, 11.0]]}))  # 책에는 없음"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 61.  83. 105.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i6zxNZS76Dhx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 11.2.3 신경망의 하위층을 학습에서 제외하기\n",
        "\n",
        "DNN 의 하위층은 이미지의 저수준 특징을 감지하므로, 다른 이미지 분류 작업에 유용함\n",
        "\n",
        "이 층들은 있는 그대로 재사용할 수 있음\n",
        "\n",
        "일반적으로 새로운 DNN을 훈련시킬 때 재사용되는 층들의 가중치는 **'동결'** 하는 것이 좋음\n",
        "(이렇게 학습되지 않는 층을 **Frozen Layer** 라고 함)\n",
        "\n",
        "\n",
        "#### 학습 과정에서 하위층 고정하는 방법\n",
        "1. 하위층의 변수를 제외하고 훈련시킬 변수 목록을 옵티마이저에 전달\n"
      ]
    },
    {
      "metadata": {
        "id": "YeRoQnGN6x2L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "9d71555b-80af-495e-e7b1-1b5f763cfcdd"
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 * 28  # MNIST\n",
        "n_hidden1 = 300 # 재사용\n",
        "n_hidden2 = 50  # 재사용\n",
        "n_hidden3 = 50  # 재사용\n",
        "n_hidden4 = 20  # 새로 만듦!\n",
        "n_outputs = 10  # 새로 만듦!\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
        "\n",
        "with tf.name_scope(\"dnn\"):\n",
        "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")       # 재사용\n",
        "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # 재사용\n",
        "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # 재사용\n",
        "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # 새로 만듦!\n",
        "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")                         # 새로 만듦!\n",
        "\n",
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
        "\n",
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
        "    \n",
        "with tf.name_scope(\"train\"):                                         # 책에는 없음\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)     # 책에는 없음\n",
        "    \n",
        "    ### 학습하려는 변수 범위만 `scope`으로 필터링\n",
        "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
        "                                   scope=\"hidden[34]|outputs\")\n",
        "    ### 이렇게 필터된 리스트만 `var_list`에 추가\n",
        "    training_op = optimizer.minimize(loss, var_list=train_vars)\n",
        "    \n",
        "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
        "                               scope=\"hidden[123]\") # 정규 표현식\n",
        "restore_saver = tf.train.Saver(reuse_vars) # 1-3층 복원\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    restore_saver.restore(sess, \"./my_model_gradients_clipping_final.ckpt\")\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "        print(epoch, \"검증 세트 정확도:\", accuracy_val)\n",
        "\n",
        "    save_path = saver.save(sess, \"./my_new_model_frozen_1_final.ckpt\")"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./my_model_gradients_clipping_final.ckpt\n",
            "0 검증 세트 정확도: 0.8962\n",
            "1 검증 세트 정확도: 0.9292\n",
            "2 검증 세트 정확도: 0.9396\n",
            "3 검증 세트 정확도: 0.9442\n",
            "4 검증 세트 정확도: 0.9482\n",
            "5 검증 세트 정확도: 0.9506\n",
            "6 검증 세트 정확도: 0.9506\n",
            "7 검증 세트 정확도: 0.9536\n",
            "8 검증 세트 정확도: 0.9554\n",
            "9 검증 세트 정확도: 0.9566\n",
            "10 검증 세트 정확도: 0.9562\n",
            "11 검증 세트 정확도: 0.9564\n",
            "12 검증 세트 정확도: 0.9566\n",
            "13 검증 세트 정확도: 0.9578\n",
            "14 검증 세트 정확도: 0.9592\n",
            "15 검증 세트 정확도: 0.9574\n",
            "16 검증 세트 정확도: 0.9574\n",
            "17 검증 세트 정확도: 0.9598\n",
            "18 검증 세트 정확도: 0.959\n",
            "19 검증 세트 정확도: 0.9602\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fp9EMJoX72EY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "2. 그래프에 `stop_gradient()` 층 추가. 이 아래 모든 층이 동결됨\n"
      ]
    },
    {
      "metadata": {
        "id": "LmkdPu7d7-_d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 * 28  # MNIST\n",
        "n_hidden1 = 300 # 재사용\n",
        "n_hidden2 = 50  # 재사용\n",
        "n_hidden3 = 50  # 재사용\n",
        "n_hidden4 = 20  # 새로 만듦!\n",
        "n_outputs = 10  # 새로 만듦!\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yLrRxIvz8RfZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope(\"dnn\"):\n",
        "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
        "                              name=\"hidden1\") # 동결층 재사용\n",
        "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
        "                              name=\"hidden2\") # 동결층 재사용\n",
        "    ### stop_gradient()로 동결하고자 하는 층 정의\n",
        "    hidden2_stop = tf.stop_gradient(hidden2)\n",
        "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu,\n",
        "                              name=\"hidden3\") # 동결하지 않고 재사용\n",
        "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu,\n",
        "                              name=\"hidden4\") # 새로 만듦!\n",
        "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\") # 새로 만듦!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TibR7x2_8ZVE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "f0b31167-fd60-4b86-b25e-e9034279153d"
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
        "\n",
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
        "\n",
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    training_op = optimizer.minimize(loss)\n",
        "    \n",
        "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
        "                               scope=\"hidden[123]\") # 정규 표현식\n",
        "restore_saver = tf.train.Saver(reuse_vars) # 1-3층 복원\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    restore_saver.restore(sess, \"./my_model_gradients_clipping_final.ckpt\")\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "        print(epoch, \"검증 세트 정확도:\", accuracy_val)\n",
        "\n",
        "    save_path = saver.save(sess, \"./my_new_model_frozen_2_final.ckpt\")"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./my_model_gradients_clipping_final.ckpt\n",
            "0 검증 세트 정확도: 0.9016\n",
            "1 검증 세트 정확도: 0.931\n",
            "2 검증 세트 정확도: 0.9434\n",
            "3 검증 세트 정확도: 0.9476\n",
            "4 검증 세트 정확도: 0.952\n",
            "5 검증 세트 정확도: 0.9524\n",
            "6 검증 세트 정확도: 0.9524\n",
            "7 검증 세트 정확도: 0.9556\n",
            "8 검증 세트 정확도: 0.9554\n",
            "9 검증 세트 정확도: 0.9558\n",
            "10 검증 세트 정확도: 0.9572\n",
            "11 검증 세트 정확도: 0.955\n",
            "12 검증 세트 정확도: 0.9572\n",
            "13 검증 세트 정확도: 0.9578\n",
            "14 검증 세트 정확도: 0.9586\n",
            "15 검증 세트 정확도: 0.9578\n",
            "16 검증 세트 정확도: 0.9564\n",
            "17 검증 세트 정확도: 0.9576\n",
            "18 검증 세트 정확도: 0.9594\n",
            "19 검증 세트 정확도: 0.9586\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2Zv0IzCt80R0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 11.2.4 동결된 층 캐싱하기\n",
        "\n",
        "동결된 층은 변하지 않기 때문에 각 훈련 샘플에 대해 가장 윗쪽의 동결된 층에서 나온 출력은 캐싱하는 것이 가능\n",
        "(5 층 중 1~2 층이 동결되었다면, 2층의 데이터를 캐싱)\n",
        "\n",
        "전체 훈련 세트에 대해 하위층을 먼저 실행한 후, 훈련 샘플($\\text{layer}_0$)을 이용한 배치를 만드는 것이 아니라, 은닉층 2($\\text{layer}_2$)의 출력을 배치로 만들어 훈련 연산에 주입"
      ]
    },
    {
      "metadata": {
        "id": "UHp709OB-Quq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 * 28  # MNIST\n",
        "n_hidden1 = 300 # 재사용\n",
        "n_hidden2 = 50  # 재사용\n",
        "n_hidden3 = 50  # 재사용\n",
        "n_hidden4 = 20  # 새로 만듦!\n",
        "n_outputs = 10  # 새로 만듦!\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
        "\n",
        "with tf.name_scope(\"dnn\"):\n",
        "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
        "                              name=\"hidden1\") # 동결층 재사용\n",
        "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
        "                              name=\"hidden2\") # 동결층 재사용 & 캐싱\n",
        "    hidden2_stop = tf.stop_gradient(hidden2)\n",
        "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu,\n",
        "                              name=\"hidden3\") # 동결하지 않고 재사용\n",
        "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu,\n",
        "                              name=\"hidden4\") # 새로 만듦!\n",
        "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\") # 새로 만듦!\n",
        "\n",
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
        "\n",
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
        "\n",
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    training_op = optimizer.minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mPCvkWCs-akp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
        "                               scope=\"hidden[123]\") # 정규 표현식\n",
        "restore_saver = tf.train.Saver(reuse_vars) # 1-3층 복원\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0mErE5bn-dp5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "6735e2ae-88a2-4f5e-8fed-f680ee0f7cd5"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "n_batches = len(X_train) // batch_size\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    restore_saver.restore(sess, \"./my_model_gradients_clipping_final.ckpt\")\n",
        "    \n",
        "    ### hidden2 까지 계산하여 cache로 저장\n",
        "    h2_cache = sess.run(hidden2, feed_dict={X: X_train})\n",
        "    h2_cache_valid = sess.run(hidden2, feed_dict={X: X_valid}) # 책에는 없음\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        shuffled_idx = np.random.permutation(len(X_train))\n",
        "        \n",
        "        ### 이미 계산된 hidden2 레이어에서 train 셋만 추출\n",
        "        hidden2_batches = np.array_split(h2_cache[shuffled_idx], n_batches)\n",
        "        y_batches = np.array_split(y_train[shuffled_idx], n_batches)\n",
        "        for hidden2_batch, y_batch in zip(hidden2_batches, y_batches):\n",
        "            # TODO: hidden2 가 placeholder가 아닌데, feed_dict로 주입이 가능한가?\n",
        "            sess.run(training_op, feed_dict={hidden2:hidden2_batch, y:y_batch})\n",
        "\n",
        "        accuracy_val = accuracy.eval(feed_dict={hidden2: h2_cache_valid, # 책에는 없음\n",
        "                                                y: y_valid})             # 책에는 없음\n",
        "        print(epoch, \"검증 세트 정확도:\", accuracy_val)                      # 책에는 없음\n",
        "\n",
        "    save_path = saver.save(sess, \"./my_new_model_caching_final.ckpt\")"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./my_model_gradients_clipping_final.ckpt\n",
            "0 검증 세트 정확도: 0.9016\n",
            "1 검증 세트 정확도: 0.931\n",
            "2 검증 세트 정확도: 0.9434\n",
            "3 검증 세트 정확도: 0.9476\n",
            "4 검증 세트 정확도: 0.952\n",
            "5 검증 세트 정확도: 0.9524\n",
            "6 검증 세트 정확도: 0.9524\n",
            "7 검증 세트 정확도: 0.9556\n",
            "8 검증 세트 정확도: 0.9554\n",
            "9 검증 세트 정확도: 0.9558\n",
            "10 검증 세트 정확도: 0.9572\n",
            "11 검증 세트 정확도: 0.955\n",
            "12 검증 세트 정확도: 0.9572\n",
            "13 검증 세트 정확도: 0.9578\n",
            "14 검증 세트 정확도: 0.9584\n",
            "15 검증 세트 정확도: 0.9578\n",
            "16 검증 세트 정확도: 0.9564\n",
            "17 검증 세트 정확도: 0.9576\n",
            "18 검증 세트 정확도: 0.9594\n",
            "19 검증 세트 정확도: 0.9584\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "R-gI5v-t_TFJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 11.2.5 상위층을 변경, 삭제, 대체하기\n",
        "\n",
        "일반적으로 기존 모델을 재사용하는 경우, 상위층은 재사용이 어려움\n",
        "- 출력 뉴런 수도 같지 않을 수 있음\n",
        "- 고수준 특성은 원본 작업에서 유용했던 특성과 다를 가능성이 큼\n",
        "\n",
        "새로운 모델 학습\n",
        "1. 먼저 복사한 모든 층을 동결\n",
        "2. 그 다음 모델을 훈련시키고 얼마나 성능이 나오는지 확인\n",
        "3. 가장 윗쪽의 은닉층 1~2개의 동결 해제하여 성능 향상 확인\n",
        "4. 그래도 좋은 성능을 얻을 수 없고 훈련 데이터가 적다면, 가장 윗쪽의 은닉층을 제거하고 1부터 반복"
      ]
    },
    {
      "metadata": {
        "id": "KRo80MC7AY6l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 11.2.6 모델 저장소\n",
        "\n",
        "모델을 어디서 찾아야 할까?\n",
        "1. 자신의 모델 카탈로그\n",
        "  - 나중에 찾기 쉽도록 잘 정리하자\n",
        "2. 텐서플로 모델 저장소 (https://github.com/tensorflow/models)\n",
        "3. 다른 프레임워크 기반 저장소\n",
        "  - 다양한 딥러닝 프레임워크 간의 모델 변환기: https://goo.gl/CndSsd\n",
        "  "
      ]
    },
    {
      "metadata": {
        "id": "AdF-g-N2A0rP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 11.2.7 비지도 사전훈련\n",
        "\n",
        "레이블된 학습 데이터가 많지 않은 문제\n",
        "\n",
        "1. 더 많은 레이블된 데이터 수집\n",
        "2. 비지도 사전훈련 (Unsupervised pretraining)\n",
        "  - RBM (Restricted Boltmann Machines)\n",
        "  - Auto-encoder\n",
        "  - 각 층은 훈련된 이전 층의 출력으로 훈련됨 - 훈견 중인 층을 제외하고 다른 모든 층은 동결\n",
        "  - 이렇게 모든 층이 훈련되면 역전파 알고리즘으로 세밀하게 튜닝\n",
        "  \n",
        "  ![fig_11-5](https://github.com/jonghoonseo/handson-ml/raw/wip/images/deep/fig_11_5.png)\n",
        "  \n",
        "  "
      ]
    },
    {
      "metadata": {
        "id": "ssq_cWj9CgjC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 11.2.8 보조 작업으로 사전훈련\n",
        "\n",
        "**레이블된 훈련 데이터를 쉽게 얻거나 생성할 수 있는 보조 작업** 에 첫번째 신경망을 훈련\n",
        "\n",
        "그리고 이 신경망의 하위층을 실제 작업을 위해 재사용\n",
        "\n",
        "하위층은 여러 문제에서 공용으로 사용 가능한 저수준 특징을 포함하는 경우가 많으므로, 쉬운 문제로 첫번째 신경망을 훈련하고, 이 산경망의 하위층을 실제 작업에 재사용\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Qq9zFeoXDghr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 11.3 고속 옵티마이저\n",
        "\n",
        "훈련 속도를 높이는 5가지 전략\n",
        "1. 연결 가중치에 좋은 **초기화 전략** 적용\n",
        "2. 좋은 **활성화 함수** 사용\n",
        "3. **배치 정규화** 사용\n",
        "4. 미리 훈련된 신경망의 일부 **재사용**\n",
        "5. **더 빠른 옵티마이저** 사용하기\n",
        "  - Momentum Optimizer\n",
        "  - Nesterov Accelerated Gradient\n",
        "  - AdaGrad\n",
        "  - RMSProp\n",
        "  - Adam Optimizer\n",
        "  \n",
        "  ![optimizer_history](http://intelliz.co.kr/wp-content/uploads/2015/07/333.jpg)\n",
        "\n",
        "(원본 출처: http://intelliz.co.kr/?p=196)"
      ]
    },
    {
      "metadata": {
        "id": "Sdf0HEdcEPjM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 11.3.1 모멘텀 최적화\n",
        "\n",
        "(원본 출처: https://icim.nims.re.kr/post/easyMath/428)\n",
        "\n",
        "Gradient Descent Optimizer 보다 개선된 알고리즘으로 이동값에 관성으로 인한 업데이트가 추가된 Optimizer이다. 다음과 같은 Gradient Descent Optimizer의 한계를 개선하는 데 효과가 있는 알고리즘이다.\n",
        "\n",
        "![limitation_of_GD](https://icim.nims.re.kr/file/70679cf085a049679f03736d7afad264.png)\n",
        "\n",
        "- global minimum이 아닌 local minimum으로 수렴\n",
        "- 최솟값을 찾아가는 도중에 미분계수가 0인 지점에서 더 이상 이동하지 않음\n",
        "\n",
        "![momentum](https://icim.nims.re.kr/file/1e87c8ef4e2649e69752301c385745c7.png)\n",
        "\n",
        "Gradient Descent Optimize의 이동량은 미분계수에 의해 결정되기 때문에 미분계수가 0이면 더 이상 업데이트가 되지 않는다.\n",
        "\n",
        "Momentum Optimizer는 다음과 같이 목적함수  $f$ 의 최솟값을 찾는다.\n",
        "\n",
        "- 초기값 $x_0$ 와 적당한 Learning rate $\\alpha$, momentum $\\beta$ 설정\n",
        "- $n \\geq 0$ 인 정수에 대해서 $a_{n+1}$ 과 $x_{n+1}$은 아래와 같이 정의\n",
        "$$a_{n+1} := \\beta \\cdot a_n + \\nabla f(x_n), a_0 := 0 \\\\\n",
        "x_{n+1} := x_n - \\alpha \\cdot a_{n+1}$$\n",
        "- (책 버전) **식 11-4: 모멘텀 알고리즘**\n",
        "  1. $\\mathbf{m} \\gets \\beta \\mathbf{m} - \\eta \\nabla_\\boldsymbol{\\theta}J(\\boldsymbol{\\theta})$\n",
        "  2. $\\boldsymbol{\\theta} \\gets \\boldsymbol{\\theta} + \\mathbf{m}$\n",
        "- 단, $\\eta$는 학습률\n",
        "- momentum $\\beta$\n",
        "  - 일반적인 값: 0.9\n",
        "  - 0이라면: Gradient Descent Optimizer 와 동일\n",
        "- 기울기 $\\nabla_\\theta J(\\theta) == \\nabla f(x_n) = 0$임에도 $a_n$에 의한 관성효과로 $x_n$은 갱신됨\n",
        "\n",
        "![problem_at_slope](https://github.com/jonghoonseo/handson-ml/raw/wip/images/deep/fig_4_7.png)\n",
        "\n",
        "경사 하강법은 입력 스케일이 매우 다르면 골짜기에서 오래 걸리는데 반해, 모멘텀은 매우 빠르게 갱신됨\n",
        "\n",
        "배치 정규화를 사용하지 않는 신경망에서는 스케일이 매우 다를 수 있음\n",
        "\n",
        "Local Optima 를 건너뛰도록 하는데에 도움\n",
        "\n",
        "경사 하강법보다 거의 항상 더 빠름"
      ]
    },
    {
      "metadata": {
        "id": "4Q1OO-owAXPA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
        "                                       momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7Ouhe3N8MRwj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 11.3.2 Nesterov Accelerated Gradient(NAG)\n",
        "\n",
        "Momentum Optimizer의 개선된 알고리즘으로 차이점은 Gradient $\\nabla f$ 를  $x_n$ 에서 계산하는 것이 아니라 관성(momentum)에 의해서 이동한  $x_n + \\beta \\cdot a_n$ 에서 계산하는 것이다. Nesterov Accelerated Gradient 알고리즘은 다음과 같다.\n",
        "\n",
        "- 초기값 $x_0$와 적당한 Learning rate $\\alpha$, momentum $\\beta$ 설정\n",
        "- $n \\geq 0$ 인 정수에 대해서 $a_{n+1}$, $x_{n+1}$은 다음과 같음\n",
        "$$a_{n+1} := \\beta \\cdot a_n + \\nabla f (x_n - \\alpha \\cdot \\beta \\cdot a_n)\\\\\n",
        "x_{n+1} := x_n - \\alpha \\cdot a_{n+1}$$\n",
        "- (책 버전) **식 11-5: 네스테로프 가속 경사 알고리즘**\n",
        "  1. $\\mathbf{m} \\gets \\beta \\mathbf{m} - \\eta \\nabla_\\boldsymbol{\\theta}J(\\boldsymbol{\\theta} + \\beta \\mathbf{m})$\n",
        "  2. $\\boldsymbol{\\theta} \\gets \\boldsymbol{\\theta} + \\mathbf{m}$\n",
        "  \n",
        "기본 모멘텀 최적화보다 거의 항상 더 빠름\n",
        "\n",
        "현재 위치가 아니라 모멘텀의 방향으로 조금 앞서서 비용 함수의 그래디언트를 계산\n",
        "\n",
        "![nesterov](https://github.com/jonghoonseo/handson-ml/raw/wip/images/deep/fig_11_6.png)"
      ]
    },
    {
      "metadata": {
        "id": "S48whSPcOfNS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
        "                                       momentum=0.9, use_nesterov=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5OZIQWfMOnVE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 11.3.3 AdaGrad\n",
        "\n",
        "![problem_at_slope](https://github.com/jonghoonseo/handson-ml/raw/wip/images/deep/fig_11_7.png)\n",
        "\n",
        "가장 가파른 차원을 따라 그래디언트 벡터의 스케일을 감소시켜 이 문제를 해결\n",
        "\n",
        "**식 11-6: AdaGrad 알고리즘**\n",
        "1. $\\mathbf{s} \\gets \\mathbf{s} + \\nabla_\\boldsymbol{\\theta}J(\\boldsymbol{\\theta}) \\otimes \\nabla_\\boldsymbol{\\theta}J(\\boldsymbol{\\theta})$\n",
        "2. $\\boldsymbol{\\theta} \\gets \\boldsymbol{\\theta} - \\eta \\, \\nabla_\\boldsymbol{\\theta}J(\\boldsymbol{\\theta}) \\oslash {\\sqrt{\\mathbf{s} + \\epsilon}}$\n",
        "\n",
        "간단한 2차방정식 문제에 대해서는 잘 작동하나, 너무 일찍 멈춰버리는 경향\n",
        "\n",
        "학습률이 너무 감소되어 전역 최적점에 도착하기 전에 알고리즘이 완전히 멈춤\n",
        "\n",
        "따라서, 간단한 작업 이외에는 사용하지 말아야 함"
      ]
    },
    {
      "metadata": {
        "id": "qcA4NBDnPzzb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 11.3.4 RMSProp\n",
        "\n",
        "AdaGrad 가 너무 빠르게 느려져서 전역 최적점에 수렴하지  못한 문제를 극복\n",
        "\n",
        "**식 11-7: RMSProp 알고리즘**\n",
        "\n",
        "1. $\\mathbf{s} \\gets \\beta \\mathbf{s} + (1 - \\beta ) \\nabla_\\boldsymbol{\\theta}J(\\boldsymbol{\\theta}) \\otimes \\nabla_\\boldsymbol{\\theta}J(\\boldsymbol{\\theta})$\n",
        "2. $\\boldsymbol{\\theta} \\gets \\boldsymbol{\\theta} - \\eta \\, \\nabla_\\boldsymbol{\\theta}J(\\boldsymbol{\\theta}) \\oslash {\\sqrt{\\mathbf{s} + \\epsilon}}$\n"
      ]
    },
    {
      "metadata": {
        "id": "gS2lgllbQFdW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 11.3.5 Adam 최적화\n",
        "\n",
        "- Adaptive Moment Estimation\n",
        "- 모멘텀과 RMSProp 의 하이브리드\n",
        "  - 모멘텀 최적화처럼 지난 그래디언트의 지수 감소 평균(expomential decaying average) 을 따름\n",
        "  - RMSProp 처럼 지난 그래디언트 제곱의 지수 감소된 평균을 따름\n",
        "  \n",
        "**식 11-8: Adam 알고리즘**\n",
        "\n",
        "1. $\\mathbf{m} \\gets \\beta_1 \\mathbf{m} - (1 - \\beta_1) \\nabla_\\boldsymbol{\\theta}J(\\boldsymbol{\\theta})$\n",
        "2. $\\mathbf{s} \\gets \\beta_2 \\mathbf{s} + (1 - \\beta_2) \\nabla_\\boldsymbol{\\theta}J(\\boldsymbol{\\theta}) \\otimes \\nabla_\\boldsymbol{\\theta}J(\\boldsymbol{\\theta})$\n",
        "3. $\\mathbf{m} \\gets \\left(\\dfrac{\\mathbf{m}}{1 - {\\beta_1}^T}\\right)$\n",
        "4. $\\mathbf{s} \\gets \\left(\\dfrac{\\mathbf{s}}{1 - {\\beta_2}^T}\\right)$\n",
        "5. $\\boldsymbol{\\theta} \\gets \\boldsymbol{\\theta} + \\eta \\, \\mathbf{m} \\oslash {\\sqrt{\\mathbf{s} + \\epsilon}}$\n",
        "\n",
        "일반적으로 다른 방법보다 빠르고 좋았음\n"
      ]
    },
    {
      "metadata": {
        "id": "0u54g6klRGB0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ByIBlR68RZ46",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 11.3.6 학습률 스케쥴링\n",
        "\n",
        "![learning_rate](https://github.com/jonghoonseo/handson-ml/raw/wip/images/deep/fig_11_8.png)\n",
        "\n",
        "학습 스케쥴\n",
        ": 훈련하는 동안 학습률을 감소시키는 전략\n",
        "초반에는 큰 값으로 크게 이동하고, 나중에는 작은 값으로 미세 조정\n",
        "- 미리 정의된 개별적인 고정 학습률\n",
        "  - 예를 들어 처음에 $\\eta_0 = 0.1$로 학습률을 지정하고, 50 에포크 후에 $\\eta_1 = 0.001$로 바꿈\n",
        "  - 잘 작동하지만, 적절한 학습률과 적당한 시점을 찾으려면 이리저리 바꿔봐야 함\n",
        "  - `piecewise_contant()` 사용\n",
        "- 성능 기반 스케쥴링\n",
        "  - 매 $N$ 스텝마다 검증 오차를 측정하고 오차가 줄어들지 않으면 $\\lambda$ 만큼 학습률 감소시킴\n",
        "- 지수 기반 스케쥴링\n",
        "  - 반복 횟수 $t$의 함수 $\\eta(t) = \\eta_0 10^{t/r}$\n",
        "  - 잘 작동하지만, $\\eta_0$ 와 $r$ 튜닝해야 함.\n",
        "- 거듭제곱 기반 스케쥴링\n",
        "  - $\\eta(t) = \\eta_0 (1+t/r)^{-c}$\n",
        "  - 하이퍼파라미터 $c$는 보통 1로 지정\n",
        "  - 지수 기반 스케쥴링과 비슷하나, 훨씬 느리게 감소함\n",
        "  \n",
        "지수 기반 방법이 선호됨\n",
        "\n",
        "AdaGrad, RMSProp, Adam 최적화: 자동으로 학습률을 감소시키므로 학습률 스케쥴링 불필요\n",
        "\n",
        "다른 알고리즘들: 지수기반 스케쥴링이나 성능 기반 스케쥴링을 사용하면 좋음\n",
        "\n",
        "(내부 자료: https://gerrit.wisefour.com/c/w4/+/20402)"
      ]
    },
    {
      "metadata": {
        "id": "EDyfRGECTOhr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 * 28  # MNIST\n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 50\n",
        "n_outputs = 10\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
        "\n",
        "with tf.name_scope(\"dnn\"):\n",
        "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
        "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
        "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
        "\n",
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
        "\n",
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
        "    \n",
        "with tf.name_scope(\"train\"):       # 책에는 없음\n",
        "    initial_learning_rate = 0.1\n",
        "    decay_steps = 10000\n",
        "    decay_rate = 1/10\n",
        "    ### global_step: 현재 훈련 반복 횟수 저장, 학습되지 않음(traninable=False)\n",
        "    global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
        "    \n",
        "    ### exponential_decay()\n",
        "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n",
        "                                               decay_steps, decay_rate)\n",
        "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
        "    training_op = optimizer.minimize(loss, global_step=global_step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n2tPG_OylBnB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "d703df61-40b5-4990-d062-dd496a5d7f27"
      },
      "cell_type": "code",
      "source": [
        "init = tf.global_variables_initializer()\n",
        "\n",
        "n_epochs = 20\n",
        "batch_size = 200\n",
        "\n",
        "learning_rates = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        learning_rates.append(learning_rate.eval(feed_dict={X: X_valid, y: y_valid}))\n",
        "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "        print(epoch, \"검증 세트 정확도:\", accuracy_val, \"learning rate: \", learning_rates[-1])\n",
        "        \n",
        "plt.plot(learning_rates)\n",
        "plt.show()"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 검증 세트 정확도: 0.964 learning rate:  0.0938642\n",
            "1 검증 세트 정확도: 0.9752 learning rate:  0.08810489\n",
            "2 검증 세트 정확도: 0.9692 learning rate:  0.08269895\n",
            "3 검증 세트 정확도: 0.979 learning rate:  0.077624716\n",
            "4 검증 세트 정확도: 0.9782 learning rate:  0.07286181\n",
            "5 검증 세트 정확도: 0.9826 learning rate:  0.06839117\n",
            "6 검증 세트 정확도: 0.983 learning rate:  0.06419482\n",
            "7 검증 세트 정확도: 0.9828 learning rate:  0.060255956\n",
            "8 검증 세트 정확도: 0.9844 learning rate:  0.05655877\n",
            "9 검증 세트 정확도: 0.9848 learning rate:  0.053088445\n",
            "10 검증 세트 정확도: 0.9844 learning rate:  0.049831044\n",
            "11 검증 세트 정확도: 0.9854 learning rate:  0.046773512\n",
            "12 검증 세트 정확도: 0.9852 learning rate:  0.043903586\n",
            "13 검증 세트 정확도: 0.9856 learning rate:  0.041209754\n",
            "14 검증 세트 정확도: 0.9846 learning rate:  0.038681205\n",
            "15 검증 세트 정확도: 0.9856 learning rate:  0.036307804\n",
            "16 검증 세트 정확도: 0.985 learning rate:  0.034080036\n",
            "17 검증 세트 정확도: 0.9842 learning rate:  0.031988952\n",
            "18 검증 세트 정확도: 0.9858 learning rate:  0.030026173\n",
            "19 검증 세트 정확도: 0.9852 learning rate:  0.02818383\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAD6CAYAAABApefCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fdfd9e7f4a8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "IJPW0VcfoN5b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 11.4 과대적합을 피하기 위한 규제 방법\n",
        "\n",
        "DNN 은 수만, 수백반개의 파라미터를 가짐 --> 매우 높은 자유도 --> +: 복잡한 데이터셋 학습 가능 / -: Overfitting 위험\n",
        "\n",
        "\n",
        "\n",
        "1. 조기 종료\n",
        "2. $\\ell_1$ 과 $\\ell_2$ 규제\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "4ByfbWKrpsQQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 11.4.2 조기 종료 (Early Stopping)\n",
        "\n",
        "검증 세트의 성능이 떨어지기 시작할 때 훈련 중지\n",
        "\n",
        "일정한 간격(예를 들면 50 스텝마다) 검증 세트로 모델 평가하여 이전의 최고 성능보다 더 나을 경우 이를 최고 성능의 스냅샷으로 저장하고,\n",
        "\n",
        "마지막 스냅샷이 저장된 이후 경과한 스텝이 threshold 를 넘으면 (최고 성능 스냅샷이 저장된 후 일정 스텝 경과 후) 훈련 종료\n",
        "\n",
        "실전에서 잘 작동함!\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "PMIBLYvBowpw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 11.4.2 $\\ell_1$ 과 $\\ell_2$ 규제\n",
        "\n",
        "$\\ell_1$ 과 $\\ell_2$ 규제를 연결 가중치에 제약 (bias 에는 적용하지 않음)\n",
        "\n",
        "참고: http://nbviewer.jupyter.org/github/ExcelsiorCJH/Hands-On-ML/blob/master/Chap04-Training_Models/Chap04-Training_Models.ipynb#4.5-%EA%B7%9C%EC%A0%9C(Regularization)%EA%B0%80-%EC%9E%88%EB%8A%94-%EC%84%A0%ED%98%95-%EB%AA%A8%EB%8D%B8\n",
        "\n",
        "비용 함수에 적절한 규제항을 추가\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "JTM2c9aGou5z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 * 28  # MNIST\n",
        "n_hidden1 = 300\n",
        "n_outputs = 10\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
        "\n",
        "with tf.name_scope(\"dnn\"):\n",
        "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
        "    logits = tf.layers.dense(hidden1, n_outputs, name=\"outputs\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ARZEt0j5pbPX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "그 다음, 층의 가중치에 대한 핸들을 얻어 크로스 엔트로피 손실에 $\\ell_1$ 손실(즉, 가중치의 절댓값)을 더해 전체 손실을 계산합니다:"
      ]
    },
    {
      "metadata": {
        "id": "sIoytj2HqcnC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### 각 레이어의 가중치 (/kernel)\n",
        "W1 = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
        "W2 = tf.get_default_graph().get_tensor_by_name(\"outputs/kernel:0\")\n",
        "\n",
        "scale = 0.001 # l1 규제 하이퍼파라미터\n",
        "\n",
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
        "                                                              logits=logits)\n",
        "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")\n",
        "    ### \n",
        "    reg_losses = tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2))\n",
        "    loss = tf.add(base_loss, scale * reg_losses, name=\"loss\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "74Rh0tW8rCtJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "나머지는 동일"
      ]
    },
    {
      "metadata": {
        "id": "uUqwyytcrEV0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "9832a59b-9895-4843-abbd-534563de0159"
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
        "\n",
        "learning_rate = 0.01\n",
        "\n",
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    training_op = optimizer.minimize(loss)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "n_epochs = 20\n",
        "batch_size = 200\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "        print(epoch, \"검증 세트 정확도:\", accuracy_val)\n",
        "\n",
        "    save_path = saver.save(sess, \"./my_model_regulation_final.ckpt\")"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 검증 세트 정확도: 0.831\n",
            "1 검증 세트 정확도: 0.871\n",
            "2 검증 세트 정확도: 0.8838\n",
            "3 검증 세트 정확도: 0.8934\n",
            "4 검증 세트 정확도: 0.8966\n",
            "5 검증 세트 정확도: 0.8988\n",
            "6 검증 세트 정확도: 0.9016\n",
            "7 검증 세트 정확도: 0.9044\n",
            "8 검증 세트 정확도: 0.9058\n",
            "9 검증 세트 정확도: 0.906\n",
            "10 검증 세트 정확도: 0.9068\n",
            "11 검증 세트 정확도: 0.9054\n",
            "12 검증 세트 정확도: 0.907\n",
            "13 검증 세트 정확도: 0.9084\n",
            "14 검증 세트 정확도: 0.9088\n",
            "15 검증 세트 정확도: 0.9064\n",
            "16 검증 세트 정확도: 0.9066\n",
            "17 검증 세트 정확도: 0.9066\n",
            "18 검증 세트 정확도: 0.9066\n",
            "19 검증 세트 정확도: 0.9052\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b2OZKSs5rSo2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "레이어마다 이런 형태로 구현하는 것은 복잡함\n",
        "\n",
        "다른 방법으로는 `tf.layers.dense()` 함수에 규제 함수를 전달할 수 있습니다. 이 함수는 규제 손실을 계산하기 위한 연산을 만들고 규제 손실 컬렉션에 이 연산을 추가합니다. 모델 선언부는 이전과 동일합니다:"
      ]
    },
    {
      "metadata": {
        "id": "6PxicC3VrtAY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 * 28  # MNIST\n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 50\n",
        "n_outputs = 10\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n_X79Wm_rwkz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "그다음, 동일한 매개변수를 매번 반복하지 않으려고 파이썬의 `partial()` 함수를 사용합니다. `kernel_regularizer` 매개변수를 지정해야 합니다:"
      ]
    },
    {
      "metadata": {
        "id": "oA3evJTqrxsH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "scale = 0.001\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "### partial 함수로 각 layer 선언. kernel_regularizer 추가!\n",
        "my_dense_layer = partial(\n",
        "    tf.layers.dense, activation=tf.nn.relu,\n",
        "    kernel_regularizer=tf.contrib.layers.l1_regularizer(scale))\n",
        "\n",
        "with tf.name_scope(\"dnn\"):\n",
        "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
        "    hidden2 = my_dense_layer(hidden1, n_hidden2, name=\"hidden2\")\n",
        "    logits = my_dense_layer(hidden2, n_outputs, activation=None,\n",
        "                            name=\"outputs\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9sxP01PAr7nM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "기본 손실에 규제 손실을 추가합니다:"
      ]
    },
    {
      "metadata": {
        "id": "QYCwCplGr9f3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope(\"loss\"):                                     # 책에는 없음\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(  # 책에는 없음\n",
        "        labels=y, logits=logits)                                # 책에는 없음\n",
        "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")   # 책에는 없음\n",
        "    ### 규제 손실 Regularization Loss\n",
        "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
        "    ### 전체 손실에 규제 손실 추가\n",
        "    loss = tf.add_n([base_loss] + reg_losses, name=\"loss\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U8uVjMh3sTg_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "나머지는 동일"
      ]
    },
    {
      "metadata": {
        "id": "0BAm7ukvsVLW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "6f0dc5fb-d09a-4b8a-c526-ad298a2a34f0"
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
        "\n",
        "learning_rate = 0.01\n",
        "\n",
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    training_op = optimizer.minimize(loss)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "n_epochs = 20\n",
        "batch_size = 200\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "        print(epoch, \"검증 세트 정확도:\", accuracy_val)\n",
        "\n",
        "    save_path = saver.save(sess, \"./my_model_regularization2_final.ckpt\")"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 검증 세트 정확도: 0.8274\n",
            "1 검증 세트 정확도: 0.8766\n",
            "2 검증 세트 정확도: 0.8952\n",
            "3 검증 세트 정확도: 0.9016\n",
            "4 검증 세트 정확도: 0.9082\n",
            "5 검증 세트 정확도: 0.9096\n",
            "6 검증 세트 정확도: 0.9126\n",
            "7 검증 세트 정확도: 0.9154\n",
            "8 검증 세트 정확도: 0.9178\n",
            "9 검증 세트 정확도: 0.919\n",
            "10 검증 세트 정확도: 0.92\n",
            "11 검증 세트 정확도: 0.9224\n",
            "12 검증 세트 정확도: 0.9212\n",
            "13 검증 세트 정확도: 0.9228\n",
            "14 검증 세트 정확도: 0.9224\n",
            "15 검증 세트 정확도: 0.9216\n",
            "16 검증 세트 정확도: 0.9218\n",
            "17 검증 세트 정확도: 0.9228\n",
            "18 검증 세트 정확도: 0.9216\n",
            "19 검증 세트 정확도: 0.9214\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z1yylz9vsows",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 11.4.3 Dropout (제외)\n",
        "\n",
        "ref. (Hinton, Srivastava, Krizhevsky, Sutskever, & Salakhutdinov, 2012), (Srivastava, Hinton, Krizhevsky, Sutskever, & Salakhutdinov, 2014)\n",
        "\n",
        "매 훈련 스텝에서 각 뉴런(입력 뉴런은 포함, 출력 뉴런은 제외)은 임시적으로 드롭아웃될 확률 $p$(드롭아웃 비율 droptou rate, 보통 $50\\%$)를 가짐. \n",
        "즉, 이번 훈련 스텝에는 완전히 무시되지만, 다음 스텝에는 활성화 가능\n",
        "\n",
        "![fig 11-9](https://github.com/jonghoonseo/handson-ml/raw/wip/images/deep/fig_11-9.png)\n",
        "\n",
        "\n",
        "----\n",
        "\n",
        "#### 왜 Dropout 이 성능을 향상하나?\n",
        "\n",
        "회사 비유\n",
        "\n",
        "출근 여부를 동전 던지기 확률로 결정\n",
        "\n",
        "모든 업무를 한 명이 전담할 수 없음\n",
        "\n",
        "전문성이 여러 사람에게 분담되어야 하며, 많은 여러 사람과 협력해야 하므로 회사의 유연성이 높아짐\n",
        "\n",
        "----\n",
        "드롭아웃으로 훈련된 뉴런은, 이웃한 뉴런에 맞추어 적응될 수 없음\n",
        "\n",
        "즉, 가능한 한 자기 자신이 유용해져야 함\n",
        "\n",
        "이런 뉴런들은 몇 개의 입력 뉴런에만 지나치게 의존할 수 없음 -> 모든 입력 뉴런에 주의 기울여야 함 -> 입력값의 작은 변화에 덜 민감 -> 더 안정적인 네트워크\n",
        "\n",
        "----\n",
        "각 훈련 스텝마다 고유 네트워크 생성\n",
        "\n",
        "개별 신경망은 대부분의 가중치를 공유하고 있기 때문에 아주 독립적이지는 않음\n",
        "\n",
        "결과적으로 만들어진 신경망은 이 모든 신경망을 평균한 앙상블로 볼 수 있음\n",
        "\n",
        "----\n",
        "\n",
        "#### 기술적 세부사항\n",
        "\n",
        "$p = 50\\%$ 인 경우\n",
        "\n",
        "평균적으로 한 뉴런이, 테스트 시에 훈련시보다 두 배 많은 입력 뉴런과 연결됨\n",
        "\n",
        "이런 점을 보상하기 위해 훈련하고 나서 각 뉴런의 연결 가중치에 0.5를 곱함\n",
        "\n",
        "##### general rule\n",
        "\n",
        "\n",
        "훈련 후에 각 입력의 연결 가중치에 keep probability $(1-p)$를 곱해야 함\n",
        "\n",
        "훈련 중에 각 뉴런의 출력을 keep probability 로 나눔\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "vT5Vpu9LwRkK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### dropout in tensorflow\n",
        "\n",
        "텐서플로를 사용해 드롭아웃을 구현하려면, `tf.layers.dropout()`함수를 입력층이나 원하는 은닉층의 출력에 적용\n",
        "\n",
        "훈련 동안 이 함수는 무작위로 **뉴런을 제거**하며 **나머지 출력을 보존 확률로 나눔**\n",
        "\n",
        "훈련 시에는 `training` 변수를 `True`로, 테스트 시에는 `False`로 지정해야 함\n",
        "\n",
        "보통 `tf.nn.dropout()` 함수보다 `tf.layers.dropout()` 함수를 더 선호함. 후자는 훈련하지 않을 때 작동하지 않으나, 전자는 훈련하지 않을 때에도 작동함."
      ]
    },
    {
      "metadata": {
        "id": "DwpuNZwUwjPd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MtzN87MVwnWP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "\n",
        "### define dropout_rate\n",
        "dropout_rate = 0.5  # == 1 - keep_prob\n",
        "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
        "\n",
        "with tf.name_scope(\"dnn\"):\n",
        "    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu,\n",
        "                              name=\"hidden1\")\n",
        "    ### insert dropout layer\n",
        "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
        "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu,\n",
        "                              name=\"hidden2\")\n",
        "    ### insert dropout layer\n",
        "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
        "    logits = tf.layers.dense(hidden2_drop, n_outputs, name=\"outputs\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1aIX8JOCxDXU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "나머지는 동일"
      ]
    },
    {
      "metadata": {
        "id": "bJ7NW-q0wyvh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "0c8aa0db-9b21-4169-aee1-ec3e42981537"
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
        "\n",
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
        "    training_op = optimizer.minimize(loss)    \n",
        "\n",
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "    \n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "n_epochs = 20\n",
        "batch_size = 50\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "            sess.run(training_op, feed_dict={training: True, X: X_batch, y: y_batch})\n",
        "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "        print(epoch, \"검증 세트 정확도:\", accuracy_val)\n",
        "\n",
        "    save_path = saver.save(sess, \"./my_model_dropout_final.ckpt\")"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 검증 세트 정확도: 0.9254\n",
            "1 검증 세트 정확도: 0.9452\n",
            "2 검증 세트 정확도: 0.9492\n",
            "3 검증 세트 정확도: 0.9566\n",
            "4 검증 세트 정확도: 0.9618\n",
            "5 검증 세트 정확도: 0.96\n",
            "6 검증 세트 정확도: 0.96\n",
            "7 검증 세트 정확도: 0.9684\n",
            "8 검증 세트 정확도: 0.9688\n",
            "9 검증 세트 정확도: 0.9712\n",
            "10 검증 세트 정확도: 0.9706\n",
            "11 검증 세트 정확도: 0.9684\n",
            "12 검증 세트 정확도: 0.9716\n",
            "13 검증 세트 정확도: 0.972\n",
            "14 검증 세트 정확도: 0.9724\n",
            "15 검증 세트 정확도: 0.972\n",
            "16 검증 세트 정확도: 0.9714\n",
            "17 검증 세트 정확도: 0.9712\n",
            "18 검증 세트 정확도: 0.9732\n",
            "19 검증 세트 정확도: 0.9732\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "h3UbX7nFx4Op",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 드롭아웃의 적용 팁\n",
        "\n",
        "- overfitting? -> increase dropout rate\n",
        "- underfitting? ->decrease dropout rate\n",
        "\n",
        "- deep network? -> increase dropout rate\n",
        "- shallow network? -> decrease dropout rate\n",
        "\n",
        "----\n",
        "#### 추가 - Dropconnect\n",
        "\n",
        "드롭아웃의 변종으로, 전체 뉴런이 아니라 개별 연결을 무작위로 끔\n",
        "\n",
        "일반적으로 드롭아웃의 성능이 더 좋음"
      ]
    },
    {
      "metadata": {
        "id": "LM-VjSwTunZv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 11.4.4 Max-Norm 규제\n",
        "\n",
        "각각의 뉴런에 대해 입력의 연결 가중치 $\\mathbf{w} $ 가 $\\left\\| \\mathbf{w} \\right\\|_2 \\leq r$이 되도록 제한\n",
        "- $r$은 맥스-노름 하이퍼파라미터\n",
        "- $\\left\\|\\cdot \\right\\|_2$는 $\\ell_2$ 노름\n",
        "\n",
        "매 훈련 스텝이 끝나고 $\\left\\| \\mathbf{w} \\right\\|_2$ 를 계산한 다음 $\\mathbf{w}$를 클리핑 $ \\left( \\mathbf{w} \\gets \\mathbf{w} \\dfrac{r}{\\left\\| \\mathbf{w} \\right\\|_2} \\right)$ 합니다.\n",
        "\n",
        "----\n",
        "\n",
        "$r$을 줄이면 -> 규제 커짐 -> overfitting 감소\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "6mDhHPhhxWFQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Max-Norm Regularization in tensorflow\n",
        "\n",
        "텐서플로에는 내장되어 있지 않음\n",
        "\n",
        "은닉층의 가중치를 구한 후, `clip_by_norm()` 함수를 사용해 각 행 벡터의 최대 노름이 1.0이 되도록 가중치를 클리핑\n"
      ]
    },
    {
      "metadata": {
        "id": "WCe9DLfAyxvI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 * 28\n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 50\n",
        "n_outputs = 10\n",
        "\n",
        "learning_rate = 0.01\n",
        "momentum = 0.9\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
        "\n",
        "with tf.name_scope(\"dnn\"):\n",
        "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
        "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
        "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
        "\n",
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
        "\n",
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
        "    training_op = optimizer.minimize(loss)    \n",
        "\n",
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ktz6eg_ZzE2s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "다음으로 첫 번째 은닉층의 가중치에 대한 핸들을 얻고 `clip_by_norm()` 함수를 사용해 가중치를 클리핑하는 연산을 만듭니다. 그런 다음 클리핑된 가중치를 가중치 변수에 할당하는 연산을 만듭니다:"
      ]
    },
    {
      "metadata": {
        "id": "d0uSSZ-xzKwP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "threshold = 1.0\n",
        "\n",
        "# 은닉층 가중치에 대한 핸들 획득\n",
        "weights = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
        "# 가중치를 클리핑\n",
        "clipped_weights = tf.clip_by_norm(weights, clip_norm=threshold, axes=1)\n",
        "# 클리핑 된 가중치를 가중치 변수에 다시 할당\n",
        "clip_weights = tf.assign(weights, clipped_weights)\n",
        "\n",
        "# 은닉층 가중치에 대한 핸들 획득\n",
        "weights2 = tf.get_default_graph().get_tensor_by_name(\"hidden2/kernel:0\")\n",
        "# 가중치를 클리핑\n",
        "clipped_weights2 = tf.clip_by_norm(weights2, clip_norm=threshold, axes=1)\n",
        "# 클리핑 된 가중치를 가중치 변수에 다시 할당\n",
        "clip_weights2 = tf.assign(weights2, clipped_weights2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uYM2TJaNzsda",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "06b4593a-35fc-4762-d82c-cae4a1320c1e"
      },
      "cell_type": "code",
      "source": [
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "n_epochs = 20\n",
        "batch_size = 50\n",
        "\n",
        "with tf.Session() as sess:                                              # 책에는 없음\n",
        "    init.run()                                                          # 책에는 없음\n",
        "    for epoch in range(n_epochs):                                       # 책에는 없음\n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):  # 책에는 없음\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "            # clip_weights, clip_weights2 를 계산\n",
        "            clip_weights.eval()\n",
        "            clip_weights2.eval()                                        # 책에는 없음\n",
        "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid}) # 책에는 없음\n",
        "        print(epoch, \"검증 세트 정확도:\", accuracy_val)                     # 책에는 없음\n",
        "\n",
        "    save_path = saver.save(sess, \"./my_model_max_norm_final.ckpt\")               # 책에는 없음"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 검증 세트 정확도: 0.9568\n",
            "1 검증 세트 정확도: 0.9696\n",
            "2 검증 세트 정확도: 0.9722\n",
            "3 검증 세트 정확도: 0.977\n",
            "4 검증 세트 정확도: 0.977\n",
            "5 검증 세트 정확도: 0.9772\n",
            "6 검증 세트 정확도: 0.9818\n",
            "7 검증 세트 정확도: 0.9824\n",
            "8 검증 세트 정확도: 0.9806\n",
            "9 검증 세트 정확도: 0.9826\n",
            "10 검증 세트 정확도: 0.9826\n",
            "11 검증 세트 정확도: 0.9836\n",
            "12 검증 세트 정확도: 0.9828\n",
            "13 검증 세트 정확도: 0.9838\n",
            "14 검증 세트 정확도: 0.9848\n",
            "15 검증 세트 정확도: 0.9846\n",
            "16 검증 세트 정확도: 0.9842\n",
            "17 검증 세트 정확도: 0.9846\n",
            "18 검증 세트 정확도: 0.985\n",
            "19 검증 세트 정확도: 0.985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KKo1tlI90kbj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "위 구현은 이해하기 쉽고 잘 작동하지만 조금 번거롭습니다. 더 나은 방법은 `max_norm_regularizer()` 함수를 만드는 것입니다:"
      ]
    },
    {
      "metadata": {
        "id": "ytHU8wqM0moe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def max_norm_regularizer(threshold, axes=1, name=\"max_norm\",\n",
        "                         collection=\"max_norm\"):\n",
        "    def max_norm(weights):\n",
        "        clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)\n",
        "        clip_weights = tf.assign(weights, clipped, name=name)\n",
        "        tf.add_to_collection(collection, clip_weights)\n",
        "        return None # 규제 손실을 위한 항이 없습니다\n",
        "    return max_norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KgiC07Wa1ets",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "그런 다음 (필요한 임계값을 지정해서) 맥스 노름 규제 매개변수에 넘길 함수를 만들기 위해 이 함수를 호출합니다. 은닉층을 만들 때 이 규제 함수를 `kernel_regularizer` 매개변수를 통해 전달할 수 있습니다:"
      ]
    },
    {
      "metadata": {
        "id": "Sj7tDmoC1my4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 * 28\n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 50\n",
        "n_outputs = 10\n",
        "\n",
        "learning_rate = 0.01\n",
        "momentum = 0.9\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "URWVPyQi1pvp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "max_norm_reg = max_norm_regularizer(threshold=1.0)\n",
        "\n",
        "with tf.name_scope(\"dnn\"):\n",
        "    # kernel_regularizer = max_norm_reg 로 규제함수 전달\n",
        "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
        "                              kernel_regularizer=max_norm_reg, name=\"hidden1\")\n",
        "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
        "                              kernel_regularizer=max_norm_reg, name=\"hidden2\")\n",
        "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ahfwQH8F19hx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
        "\n",
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
        "    training_op = optimizer.minimize(loss)    \n",
        "\n",
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "n_epochs = 20\n",
        "batch_size = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8aOw_WpT2C6g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "3040088e-cd64-400c-ac33-4b994a4952e0"
      },
      "cell_type": "code",
      "source": [
        "# max_norm 들에 대한 collection\n",
        "clip_all_weights = tf.get_collection(\"max_norm\")\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "            # max_norm collection 실행\n",
        "            sess.run(clip_all_weights)\n",
        "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid}) # 책에는 없음\n",
        "        print(epoch, \"검증 세트 정확도:\", accuracy_val)                      # 책에는 없음\n",
        "\n",
        "    save_path = saver.save(sess, \"./my_model_max_norm2_final.ckpt\")                # 책에는 없음"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 검증 세트 정확도: 0.9556\n",
            "1 검증 세트 정확도: 0.9702\n",
            "2 검증 세트 정확도: 0.9694\n",
            "3 검증 세트 정확도: 0.9734\n",
            "4 검증 세트 정확도: 0.9776\n",
            "5 검증 세트 정확도: 0.9758\n",
            "6 검증 세트 정확도: 0.9806\n",
            "7 검증 세트 정확도: 0.9812\n",
            "8 검증 세트 정확도: 0.9832\n",
            "9 검증 세트 정확도: 0.9826\n",
            "10 검증 세트 정확도: 0.9818\n",
            "11 검증 세트 정확도: 0.9828\n",
            "12 검증 세트 정확도: 0.9828\n",
            "13 검증 세트 정확도: 0.9836\n",
            "14 검증 세트 정확도: 0.9838\n",
            "15 검증 세트 정확도: 0.984\n",
            "16 검증 세트 정확도: 0.9832\n",
            "17 검증 세트 정확도: 0.9838\n",
            "18 검증 세트 정확도: 0.9844\n",
            "19 검증 세트 정확도: 0.9844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ipPbql6s21rL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 11.4.5 Data Augmentation\n",
        "\n",
        "기존의 데이터에서 새로운 데이터를 생성해 인공적으로 훈련 세트의 크기를 늘림\n",
        "\n",
        "이상적으로는 사람이 인공적으로 만든 샘프인지 아닌지 구분할 수 없어야 함\n",
        "\n",
        "단순한 white noise 를 추가하는 것은 도움이 되지 않음\n",
        "\n",
        "예) transpose, rotate, resize, flip, crop"
      ]
    },
    {
      "metadata": {
        "id": "sL1We8fx4Yh1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 11.5 실용적 가이드라인\n",
        "\n",
        "기본 DNN 설정\n",
        "\n",
        "| 항목 | 알고리즘 |\n",
        "|---|---|\n",
        "| 초기화 | He Initialization |\n",
        "| 활성화 함수 | ELU |\n",
        "| 정규화 | Batch Normalization |\n",
        "| 규제 | Dropout |\n",
        "| 옵티마이저 | Nesterov Accelerated Gradient(NAG) |\n",
        "|학습률 스케쥴링 | N/A |\n",
        "\n",
        "다음 경우에는 위의 설정 변경 필요\n",
        "\n",
        "- 좋은 learning rate를 찾을 수 없을 때 ==> 지수 감소 같은 [학습 스케쥴](https://colab.research.google.com/drive/155vfvxdnEEeq-nvBrGYgx2_FEWBllpXt#scrollTo=ByIBlR68RZ46) 추가\n",
        "- 훈련 세트가 너무 작으면 ==> [Data Augmentation](https://colab.research.google.com/drive/155vfvxdnEEeq-nvBrGYgx2_FEWBllpXt#scrollTo=ipPbql6s21rL&line=9&uniqifier=1)\n",
        "- Sparse 모델이 필요하면 ==> [$\\ell_1$ 규제](https://colab.research.google.com/drive/155vfvxdnEEeq-nvBrGYgx2_FEWBllpXt#scrollTo=PMIBLYvBowpw) 추가\n",
        "- 실행 속도가 아주 빠른 모델 필요하면 ==> Batch Normalization 제거, ELU 대신 [LeakyReLU](https://colab.research.google.com/drive/155vfvxdnEEeq-nvBrGYgx2_FEWBllpXt#scrollTo=S4Hp5UOeAQPb)로 변경, Sparse 모델 생성"
      ]
    },
    {
      "metadata": {
        "id": "vK3cs2KPsugC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### References\n",
        "\n",
        "(Hinton, Srivastava, Krizhevsky, Sutskever, & Salakhutdinov, 2012) Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. R. (2012). Improving neural networks by preventing co-adaptation of feature detectors. Retrieved (으)로부터 https://arxiv.org/abs/1207.0580\n",
        "\n",
        "(Srivastava, Hinton, Krizhevsky, Sutskever, & Salakhutdinov, 2014) Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Journal of Machine Learning Research, 15, 1929–1958.\n",
        "\n"
      ]
    }
  ]
}